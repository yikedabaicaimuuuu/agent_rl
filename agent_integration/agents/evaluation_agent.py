from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from ragas import EvaluationDataset, evaluate
from ragas.metrics import (
    ContextPrecision,
    LLMContextRecall,
    Faithfulness,
    ResponseRelevancy,
    NoiseSensitivity
)
from ragas.llms import LangchainLLMWrapper

import os
import traceback
try:
    import numpy as _np
except Exception:
    _np = None  # æ²¡è£… numpy ä¹Ÿè¦å®šä¹‰ï¼Œé¿å… NameError
import re
import numbers
import math



class EvaluationAgent:
    """
    Multi-functional evaluation Agent:
      1) quick_evaluate(...) â€” åˆ¤æ–­æ£€ç´¢æ˜¯å¦è¶³ä»¥å›ç­”é—®é¢˜
      2) evaluate_retrieval(...) â€” ä»…è¯„ä¼°æ£€ç´¢è´¨é‡ï¼ˆPrecision/Recallï¼‰
      3) evaluate_generation(...) â€” ä»…è¯„ä¼°ç”Ÿæˆè´¨é‡ï¼ˆä¸€æ¬¡ evaluate è·‘å¤šæŒ‡æ ‡ï¼‰
      4) full_evaluate(...) â€” å…¼å®¹å¼ä¸€æ¬¡æ€§è¯„ä¼°ï¼ˆæ£€ç´¢+faithfulnessï¼‰
    """

    # ======================= å·¥å…·å‡½æ•°ï¼ˆä¿ç•™å¿…è¦çš„ä¸¤æšï¼‰ =======================

    @staticmethod
    def _get_numeric_value(value):
        """æŠ½å–æ•°å€¼ï¼ˆæ”¯æŒ numpy / list å‡å€¼ / å¯¹è±¡.valueï¼‰"""
        if value is None:
            return 0.0
        if isinstance(value, numbers.Real):
            return float(value)
        # 2) numpyï¼ˆå¯é€‰ï¼‰ï¼šæ ‡é‡/æ•°ç»„
        if _np is not None:
            if isinstance(value, (_np.floating, _np.integer)):
                return float(value)
            if isinstance(value, _np.ndarray):
                try:
                    return float(value.mean())
                except Exception:
                    pass

        # 3) åºåˆ—ï¼šlist/tuple -> å‡å€¼
        if isinstance(value, (list, tuple)):
            nums = []
            for v in value:
                try:
                    nums.append(float(v))
                except Exception:
                    pass
            return (sum(nums) / len(nums)) if nums else 0.0

        # 4) å…œåº•ï¼šfloat(...) æˆ–å¯¹è±¡.value
        try:
            return float(value)
        except (TypeError, ValueError):
            if hasattr(value, "value") and isinstance(getattr(value, "value"), numbers.Real):
                return float(getattr(value, "value"))
            return 0.0



    @staticmethod
    def _extract_score(result, metric_name):
        """
        å…¼å®¹ä¸åŒ ragas ç»“æ„ï¼šdict / .scores / .data / list(item.name, item.score)
        """
        # dict
        if isinstance(result, dict) and metric_name in result:
            return EvaluationAgent._get_numeric_value(result[metric_name])

        # å¯ä¸‹æ ‡ï¼ˆä½†ä¸æŠŠ str å½“æˆå¯ä¸‹æ ‡å®¹å™¨ï¼‰
        if hasattr(result, "__getitem__") and not isinstance(result, str):
            try:
                return EvaluationAgent._get_numeric_value(result[metric_name])
            except (KeyError, TypeError, IndexError):
                pass

        # .scores
        if hasattr(result, "scores"):
            scores = getattr(result, "scores")
            if isinstance(scores, dict) and metric_name in scores:
                return EvaluationAgent._get_numeric_value(scores[metric_name])

        # .data
        if hasattr(result, "data") and isinstance(getattr(result, "data"), dict):
            data = getattr(result, "data")
            if metric_name in data:
                return EvaluationAgent._get_numeric_value(data[metric_name])

        # list[MetricResult] / å¯è¿­ä»£ï¼ˆæ’é™¤ str/dictï¼‰
        if hasattr(result, "__iter__") and not isinstance(result, (str, dict)):
            try:
                for item in result:
                    name = getattr(item, "name", None) or getattr(item, "metric", None)
                    if name is None and hasattr(item, "metric") and hasattr(item.metric, "name"):
                        name = getattr(item.metric, "name")
                    if name and str(name) == str(metric_name):
                        val = getattr(item, "score", None)
                        if val is None and hasattr(item, "value"):
                            val = getattr(item, "value")
                        return EvaluationAgent._get_numeric_value(val)
            except Exception:
                pass

        # æœªå‘½ä¸­
        return 0.0



    @staticmethod
    def _num_with_status(x, default=0.0):
        """åŸºäºä½ å·²æœ‰ _get_numeric_valueï¼Œè¡¥ä¸Š none/nan/error çš„çŠ¶æ€è¯­ä¹‰ã€‚"""
        try:
            if x is None:
                return default, "none"

            # list/tuple/numpy.ndarrayï¼šå‡å€¼
            if isinstance(x, (list, tuple)) or (_np is not None and isinstance(x, _np.ndarray)):
                nums = []
                seq = x.tolist() if (_np is not None and hasattr(x, "tolist")) else x
                for v in seq:
                    try:
                        nums.append(float(v))
                    except Exception:
                        pass
                if not nums:
                    return default, "error"
                f = sum(nums) / len(nums)
                return (default, "nan") if math.isnan(f) else (f, "ok")

            # å…¶ä½™ç›´æ¥èµ° _get_numeric_value
            f = float(EvaluationAgent._get_numeric_value(x))
            return (default, "nan") if math.isnan(f) else (f, "ok")
        except Exception:
            return default, "error"


    @staticmethod
    def _extract_score2(result, metric_names):
        """
        åœ¨ä½ å·²æœ‰ _extract_score çš„åŸºç¡€ä¸Šï¼Œæ–°å¢ï¼š
        - åŒæ—¶å°è¯•å¤šä¸ªå€™é€‰é”®ï¼ˆå¦‚ ['response_relevancy','answer_relevancy']ï¼‰
        - å…¼å®¹ list[MetricResult]ï¼ˆname/metric.nameï¼‰
        - repr å…œåº•ï¼Œæ”¯æŒè´Ÿå·/ç§‘å­¦è®¡æ•°æ³•/è·¨è¡Œ
        è¿”å›: (found: bool, value: float)
        """
        names = metric_names if isinstance(metric_names, (list, tuple)) else [metric_names]
        names = [str(n) for n in names]

        # 1) å…ˆå¤ç”¨ _extract_scoreï¼ˆé€åå°è¯•ï¼‰
        for nm in names:
            try:
                val = EvaluationAgent._extract_score(result, nm)
                # _extract_score æ‰¾ä¸åˆ°å³ 0.0ï¼›æ­¤å¤„åªæœ‰é 0 æ‰ç›´æ¥è¿”å›
                if val != 0.0:
                    return True, float(val)
            except Exception:
                pass

        # 2) å†åŠ ä¸€éï¼šä¸“é—¨å¤„ç† list[MetricResult]ï¼ˆæ›´ç¨³å¥ï¼‰
        if isinstance(result, (list, tuple)):
            for item in result:
                name = getattr(item, "name", None) or getattr(item, "metric", None)
                if name is None and hasattr(item, "metric") and hasattr(item.metric, "name"):
                    name = getattr(item.metric, "name")
                if name and str(name) in names:
                    val = getattr(item, "score", None)
                    if val is None and hasattr(item, "value"):
                        val = getattr(item, "value")
                    return True, EvaluationAgent._get_numeric_value(val)

        # 3) repr å…œåº•ï¼ˆ-1.23 / 1e-3 / è·¨è¡Œï¼›å« noise_sensitivity(...)ï¼‰
        rep = repr(result)
        num_pat = r"([-+]?\d+(?:\.\d+)?(?:[eE][-+]?\d+)?)"
        for nm in names:
            # ç²¾ç¡®é”®å
            pat_exact = rf"['\"]{re.escape(str(nm))}['\"]\s*:\s*{num_pat}"
            m = re.search(pat_exact, rep, flags=re.S)
            if m:
                try:
                    return True, float(m.group(1))
                except Exception:
                    pass
            # å®½æ¾å‰ç¼€ï¼šnoise_sensitivity(...) å˜ä½“
            if "noise_sensitivity" in str(nm):
                pat_ns = rf"['\"](noise_sensitivity[^'\"]*)['\"]\s*:\s*{num_pat}"
                m2 = re.search(pat_ns, rep, flags=re.S)
                if m2:
                    try:
                        return True, float(m2.group(2))
                    except Exception:
                        pass

        return False, 0.0

    # ======================= åˆå§‹åŒ–ï¼ˆäº‘ç«¯ä¼˜å…ˆï¼Œå…¼å®¹æœ¬åœ°ï¼‰ =======================

    def __init__(self, model_name="gpt-3.5-turbo", embeddings=None, llm=None):
        # --- LLMï¼šä¼˜å…ˆçœŸå®äº‘ç«¯ KEYï¼Œå…œåº•æœ¬åœ° ---
        if llm is not None:
            self.llm = llm
        else:
            real_key = os.getenv("OPENAI_API_KEY_REAL")
            if real_key:
                self.llm = ChatOpenAI(
                    model=os.getenv("EVAL_LLM_MODEL", "gpt-3.5-turbo"),
                    base_url="https://api.openai.com/v1",
                    api_key=real_key,
                    temperature=0.0,
                    top_p=1.0,
                    max_tokens=int(os.getenv("EVAL_MAX_TOKENS", "512")),
                    timeout=float(os.getenv("EVAL_TIMEOUT", "90")),
                    max_retries=int(os.getenv("EVAL_MAX_RETRIES", "0")),
                )
            else:
                self.llm = ChatOpenAI(
                    model=model_name,
                    base_url=os.getenv("OPENAI_API_BASE", "http://127.0.0.1:8000/v1"),
                    api_key=os.getenv("OPENAI_API_KEY", "sk-fake"),
                    temperature=0.0,
                    top_p=1.0,
                    max_tokens=int(os.getenv("EVAL_MAX_TOKENS", "512")),
                    timeout=float(os.getenv("EVAL_TIMEOUT", "90")),
                    max_retries=int(os.getenv("EVAL_MAX_RETRIES", "0")),
                )

        # --- Embeddingsï¼šç”¨äº ResponseRelevancyï¼Œå»ºè®®èµ°å®˜æ–¹ ---
        if embeddings is None:
            try:
                print("ğŸ“¦ Initializing OpenAI embeddings for ResponseRelevancy")
                self.embeddings = OpenAIEmbeddings(
                    model=os.getenv("EVAL_EMBED_MODEL", "text-embedding-3-small"),
                    api_key=os.environ.get("OPENAI_API_KEY_REAL") or os.environ.get("OPENAI_API_KEY"),
                    base_url="https://api.openai.com/v1",
                )
                emb_model = getattr(self.embeddings, "model", None) or getattr(self.embeddings, "model_name", None)
                print(f"ğŸ” Ragas/ResponseRelevancy embeddings ready: {emb_model}")

            except Exception as e:
                print(f"âš ï¸ Could not initialize embeddings: {str(e)}")
                print("âš ï¸ ResponseRelevancy will be skipped")
                self.embeddings = None
        else:
            self.embeddings = embeddings

        # --- æ‰“å° LLM çš„æ¨¡å‹åå’Œ base_urlï¼Œæ›´ç›´è§‚ ---
        try:
            name = getattr(self.llm, "model_name", None) or getattr(self.llm, "model", None)
            base = getattr(self.llm, "base_url", None)
            if base is None and hasattr(self.llm, "client"):
                base = getattr(self.llm.client, "base_url", None)
            print(f"[EvaluationAgent] llm={name} base={base}")
        except Exception:
            print("[EvaluationAgent] llm ready (could not introspect fields)")


        # full_evaluate é»˜è®¤æŒ‡æ ‡
        self.metrics = [ContextPrecision(), LLMContextRecall(), Faithfulness()]

    # ============================== 1) quick_evaluate ==============================

    def quick_evaluate(self, question, docs):
        """
        ä»…ç”¨ LLM ç²—è¯„ï¼šæ£€ç´¢æ˜¯å¦è¶³ä»¥å›ç­”é—®é¢˜ + ç»™å‡ºå…³é”®è¯å»ºè®®
        """
        if not docs:
            return {"sufficient": False, "suggested_keywords": "expand keywords"}

        parts = []
        for d in docs:
            txt = getattr(d, "page_content", "") or ""
            if txt.strip():
                parts.append(txt)
        context = " ".join(parts)

        prompt = (
            f"Question: {question}\n"
            f"Retrieved content: {context[:1000]}...\n"
            "Is this information sufficient to answer the question? "
            "Please respond with 'sufficient' or 'insufficient' and provide additional keywords."
        )
        msg = self.llm.invoke(prompt)
        text = getattr(msg, "content", str(msg)) or ""
        sufficient = "sufficient" in text.lower()
        suggested_keywords = " ".join(text.split()[-3:]) if text else ""

        return {"sufficient": sufficient, "suggested_keywords": suggested_keywords}

    # ============================== 2) evaluate_retrieval ==============================

    def evaluate_retrieval(self, user_query, retrieved_docs, reference=None):
        """
        ä¸“ç”¨äº RetrieverAgentï¼šåªè¯„ä¼° ContextPrecision / LLMContextRecall
        â€”â€” æŒ‰æ•™ç¨‹é”®åï¼šuser_input / retrieved_contexts / response / reference

        æ³¨æ„ï¼šå½“æ²¡æœ‰ reference æ—¶ï¼Œè·³è¿‡éœ€è¦ reference çš„æŒ‡æ ‡ï¼ˆå¦‚ ContextPrecisionï¼‰ï¼Œ
        åªè¿”å›åŸºç¡€æ£€ç´¢ä¿¡æ¯ã€‚
        """
        contexts = [
            getattr(doc, "page_content", "")
            for doc in retrieved_docs
            if (getattr(doc, "page_content", "") or "").strip()
        ]
        if not contexts:
            contexts = ["N/A"]

        has_reference = bool(reference and str(reference).strip())

        # å¦‚æœæ²¡æœ‰ referenceï¼Œç›´æ¥è¿”å›åŸºç¡€ä¿¡æ¯ï¼Œä¸åš ragas è¯„ä¼°
        # å› ä¸º ContextPrecision ç­‰æŒ‡æ ‡éœ€è¦ reference
        if not has_reference:
            print(f"ğŸ” [Retrieval] No reference provided, skipping ragas evaluation")
            print(f"ğŸ” [Retrieval] Retrieved {len(contexts)} contexts")
            return {
                "context_precision": 0.5,  # é»˜è®¤ä¸­ç­‰åˆ†æ•°
                "context_recall": 0.5,     # é»˜è®¤ä¸­ç­‰åˆ†æ•°
                "doc_count": len(contexts),
            }

        record = {
            "user_input": user_query,
            "retrieved_contexts": contexts,
            "response": "N/A",
            "reference": reference,
        }
        dataset = EvaluationDataset.from_list([record])

        # ---- æŒ‡æ ‡é€‰æ‹© ----
        metrics = [ContextPrecision(), LLMContextRecall()]

        try:
            result = evaluate(dataset=dataset, metrics=metrics, llm=LangchainLLMWrapper(self.llm))
            print("\nğŸ” Retrieval Eval Raw â", result)

            # è¯»å–åˆ†æ•°ï¼ˆå¤ç”¨ä½ çš„å·¥å…·å‡½æ•°ï¼‰
            scores = getattr(result, "scores", None)
            if isinstance(scores, dict):
                context_precision = self._get_numeric_value(scores.get("context_precision", 0))
                context_recall = self._get_numeric_value(scores.get("context_recall", 0))
            else:
                context_precision = self._get_numeric_value(self._extract_score(result, "context_precision"))
                context_recall = self._get_numeric_value(self._extract_score(result, "context_recall"))

            # æ‰“å°
            print(f"ğŸ¯ Context Precision: {context_precision:.4f}")
            print(f"ğŸ“ˆ Context Recall: {context_recall:.4f}")

        except Exception as e:
            print(f"âŒ Error in evaluate_retrieval: {str(e)}")
            traceback.print_exc()
            context_precision = 0.5
            context_recall = 0.5

        return {
            "context_precision": context_precision,
            "context_recall": context_recall,
            "doc_count": len(contexts),
        }

    # ============================== 3) evaluate_generation ==============================

    def evaluate_generation(self, user_query, retrieved_docs, response, reference=None):
        """
        å•æ¬¡ ragas.evaluate åŒæ—¶è·‘ Faithfulness + (å¯é€‰)ResponseRelevancy + NoiseSensitivityï¼Œ
        å¹¶è¿”å›æ¯ä¸ªæŒ‡æ ‡çš„åˆ†æ•° + statusï¼ˆok/none/nan/error/missing/disabledï¼‰ã€‚
        å…¼å®¹ Ragas ä¸åŒç‰ˆæœ¬çš„è¿”å›ï¼šdict / EvaluationResult.scores / list[MetricResult] / ä»… repr å¯è¯»ã€‚
        """
        # ---- å‡†å¤‡æ•°æ®ï¼ˆåšé•¿åº¦ä¿æŠ¤ï¼‰----
        contexts = [
            getattr(doc, "page_content", "") for doc in (retrieved_docs or [])
            if (getattr(doc, "page_content", "") or "").strip()
        ]
        if not contexts:
            contexts = ["N/A"]
        else:
            contexts = [c[:1500] for c in contexts[:2]]

        resp = (str(response) if response else "N/A")[:3000]
        data = {
            "user_input": user_query,
            "retrieved_contexts": contexts,
            "response": resp,
            "reference": reference if (reference and str(reference).strip()) else None,
        }
        dataset = EvaluationDataset.from_list([data])

        lc = LangchainLLMWrapper(self.llm)

        metrics = [Faithfulness()]
        if getattr(self, "embeddings", None) is not None:
            metrics.append(ResponseRelevancy(embeddings=self.embeddings, llm=lc))
        metrics.append(NoiseSensitivity(llm=lc))

        # é»˜è®¤åˆ†æ•°ä¸statusï¼ˆåŒºåˆ†ç¼ºå¤± vs çœŸ0ï¼‰
        out = {
            "faithfulness": 0.0, "faithfulness_status": "missing",
            "response_relevancy": 0.0,
            "response_relevancy_status": ("disabled" if getattr(self, "embeddings", None) is None else "missing"),
            "noise_sensitivity": 1.0, "noise_sensitivity_status": "missing",
            "raw": None,
        }

        try:
            # â€”â€” ä¸€æ¬¡ evaluate â€”â€”
            result = evaluate(dataset=dataset, metrics=metrics, llm=lc)
            out["raw"] = result
            print("\nğŸ” Gen Eval Raw â", type(result), repr(result))

            # å–ä¸€ä¸ªâ€œå¯è§£æä½“â€
            scores_obj = getattr(result, "scores", None) or result

            # ç»Ÿä¸€æŠ“åˆ†ï¼šä¼˜å…ˆç”¨ä½ æ–°å¢çš„ _extract_score2ï¼›æ‹¿åˆ°å€¼åç”¨ _num_with_status èµ‹çŠ¶æ€
            def grab(keys, default):
                found, val = type(self)._extract_score2(scores_obj, keys)
                if found:
                    return type(self)._num_with_status(val, default)
                else:
                    return default, "missing"

            # faithfulness
            f_val, f_st = grab(["faithfulness"], 0.0)

            # response_relevancy / answer_relevancy
            if getattr(self, "embeddings", None) is not None:
                r_val, r_st = grab(["response_relevancy", "answer_relevancy"], 0.0)
            else:
                r_val, r_st = 0.0, "disabled"

            # noise_sensitivityï¼ˆåå­—å¯èƒ½å¸¦æ‹¬å·ï¼‰
            n_val, n_st = grab(
                ["noise_sensitivity", "noise_sensitivity(mode=relevant)", "noise_sensitivity(relevant)"],
                1.0
            )

            out.update({
                "faithfulness": f_val, "faithfulness_status": f_st,
                "response_relevancy": r_val, "response_relevancy_status": r_st,
                "noise_sensitivity": n_val, "noise_sensitivity_status": n_st,
            })

            out["answer_relevancy"] = out["response_relevancy"]
            out["answer_relevancy_status"] = out["response_relevancy_status"]

            # å‹å¥½æ‰“å°
            if isinstance(scores_obj, dict):
                keys_view = list(scores_obj.keys())
            elif hasattr(scores_obj, "scores") and isinstance(scores_obj.scores, dict):
                keys_view = list(scores_obj.scores.keys())
            else:
                keys_view = f"[no dict-like keys; parsed from repr: {repr(result)[:120]}...]"

            print(f"ğŸ”‘ Parsed score keys (or repr hint): {keys_view}")
            print(
                f"âœ… Faith={out['faithfulness']:.4f}({out['faithfulness_status']}), "
                f"Rel={out['response_relevancy']:.4f}({out['response_relevancy_status']}), "
                f"Noise={out['noise_sensitivity']:.4f}({out['noise_sensitivity_status']})"
            )

        except Exception as e:
            print(f"âŒ evaluate_generation failed: {e}")

        return out



    # ============================== 4) full_evaluate ==============================

    def full_evaluate(self, query, retrieved_docs, response=None, reference=None):
        """
        ä¸€æ¬¡æ€§è¯„ä¼°ï¼ˆä¸æ—§é€»è¾‘å…¼å®¹ï¼‰ï¼š
        - Retrieval: ContextPrecision / LLMContextRecall
        - Generation: Faithfulnessï¼ˆè‹¥æœ‰ referenceï¼‰
        â€”â€” ä»æŒ‰æ•™ç¨‹é”®åæ„é€ è®°å½•
        """
        contexts = [
            getattr(doc, "page_content", "")
            for doc in retrieved_docs
            if (getattr(doc, "page_content", "") or "").strip()
        ] or ["N/A"]

        resp_text = str(response) if response is not None else "N/A"

        record = {
            "user_input": query,
            "retrieved_contexts": contexts,
            "response": resp_text,
            "reference": reference if (reference and str(reference).strip()) else None,
        }

        print(f"\nğŸ” DEBUG: Query length: {len(query)}")
        print(f"ğŸ” DEBUG: Retrieved docs: {len(contexts)}")
        print(f"ğŸ” DEBUG: Response length: {len(resp_text)}")

        dataset = EvaluationDataset.from_list([record])

        # åŠ¨æ€ metricsï¼šæ—  reference ä¸è¯„ Faithfulness
        metrics = [ContextPrecision(), LLMContextRecall()]
        if record["reference"] is not None:
            metrics.append(Faithfulness())

        try:
            result = evaluate(dataset=dataset, metrics=metrics, llm=LangchainLLMWrapper(self.llm))
            print("\nğŸ” Full Eval Raw â", result)

            scores = getattr(result, "scores", None)
            if not scores:
                print("âŒ Scores object is empty or None")
                return {"faithfulness": 0.0, "context_recall": 0.0, "context_precision": 0.0}

            faithfulness_score = self._get_numeric_value(scores.get("faithfulness", 0.0))
            context_recall = self._get_numeric_value(scores.get("context_recall", 0.0))
            context_precision = self._get_numeric_value(scores.get("context_precision", 0.0))

        except Exception as e:
            print(f"âŒ Detailed error in evaluation: {str(e)}")
            traceback.print_exc()
            faithfulness_score = 0.0
            context_recall = 0.0
            context_precision = 0.0

        print(f"ğŸ“Š Faithfulness: {faithfulness_score:.4f}")
        print(f"ğŸ“ˆ Context Recall: {context_recall:.4f}")
        print(f"ğŸ¯ Context Precision: {context_precision:.4f}")

        return {
            "faithfulness": faithfulness_score,
            "context_recall": context_recall,
            "context_precision": context_precision
        }

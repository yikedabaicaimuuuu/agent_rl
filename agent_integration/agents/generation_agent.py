# agents/generation_agent.py
from typing import Optional, Dict, Any, List, Union
import os, time, sys
import numpy as np
import dspy
from langchain_openai import ChatOpenAI

# å…è®¸ä»é¡¹ç›®æ ¹å¯¼å…¥ utils
import os as _os
sys.path.append(_os.path.abspath(_os.path.join(_os.path.dirname(__file__), '..')))

# å·¥å…·
from utils.text_utils import safe_trim_prompt, trim_text_to_token_limit
from utils.trajectory_logger import TrajectoryLogger


NumberLike = Union[int, float, np.floating, np.generic, str, List[Any]]


def extract_scalar(val: NumberLike) -> float:
    """å°† ragas/è¯„ä¼°è¿”å›çš„å„ç§ç±»å‹ç¨³å¥è½¬æˆ floatã€‚"""
    if val is None:
        return 0.0
    # list: å–å‡å€¼ï¼ˆæ›´ç¨³ï¼‰ï¼Œå¦‚æœéœ€è¦å–é¦–å…ƒç´ æ”¹æˆ float(val[0])
    if isinstance(val, list):
        nums = []
        for v in val:
            try:
                nums.append(float(v))
            except Exception:
                pass
        return float(np.mean(nums)) if nums else 0.0
    # numpy / int / float / str
    try:
        return float(val)
    except Exception:
        # dict.value å…œåº•
        if hasattr(val, "value"):
            try:
                return float(getattr(val, "value"))
            except Exception:
                return 0.0
        return 0.0


def _get_doc_text(d) -> str:
    """ä»ä»»æ„ doc æŠ½æ–‡æœ¬å†…å®¹ã€‚"""
    if d is None:
        return ""
    txt = getattr(d, "page_content", None)
    if isinstance(txt, str):
        return txt
    if isinstance(d, dict):
        for k in ("page_content", "text", "content"):
            v = d.get(k)
            if isinstance(v, str):
                return v
    if isinstance(d, str):
        return d
    return ""


class GenerationAgent:
    def __init__(
        self,
        model_name: Optional[str] = None,
        llm: Optional[ChatOpenAI] = None,
        semantic_f1_metric=None,
        logger: Optional[TrajectoryLogger] = None,
        max_ctx_tokens: int = 1400,
        max_gen_tokens: int = 512
    ):
        """
        - äº‘ç«¯ä¼˜å…ˆï¼šOPENAI_API_KEY_REAL â†’ å®˜æ–¹ç«¯ç‚¹ï¼›å¦åˆ™å›è½æœ¬åœ° OPENAI_API_BASEã€‚
        - æ§åˆ¶ä¸Šä¸‹æ–‡/ç”Ÿæˆé•¿åº¦ï¼Œé€‚é…è½»é‡èµ„æºã€‚
        """
        self.logger = logger
        self.semantic_f1_metric = semantic_f1_metric

        # è½»é‡æ¨¡å¼å¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–
        LIGHT_MODE = os.getenv("LIGHT_MODE", "0") == "1"
        if LIGHT_MODE:
            max_ctx_tokens = int(os.getenv("GEN_MAX_CTX_TOKENS", max_ctx_tokens))
            max_gen_tokens = int(os.getenv("GEN_MAX_GEN_TOKENS", max_gen_tokens))
        self.max_ctx_tokens = max_ctx_tokens
        self.max_gen_tokens = max_gen_tokens

        if llm is not None:
            self.llm = llm
        else:
            # äº‘ç«¯ä¼˜å…ˆ
            real_key = os.getenv("OPENAI_API_KEY_REAL")
            if real_key:
                base_url = "https://api.openai.com/v1"
                api_key = real_key
            else:
                base_url = os.getenv("OPENAI_API_BASE", "http://127.0.0.1:8000/v1")
                api_key = os.getenv("OPENAI_API_KEY", "sk-fake")

            default_model = "gpt-3.5-turbo"
            model_name = model_name or os.getenv("GEN_LLM_MODEL", default_model)

            self.llm = ChatOpenAI(
                model=model_name,                 # æ–°ç‰ˆç”¨ model=
                base_url=base_url,
                api_key=api_key,
                temperature=0.0,
                max_tokens=self.max_gen_tokens,
                top_p=1.0,
                max_retries=int(os.getenv("LC_MAX_RETRIES", "1")),
                timeout=float(os.getenv("LC_TIMEOUT", "60")),
            )

        # === æ›´ç¨³å¥çš„ LLM æ ‡è¯†ï¼ˆé¿å… Noneï¼‰ ===  [æ–°å¢]
        def _llm_ident(llm_obj):
            name = None
            base = None
            try:
                # å¸¸è§å­—æ®µ
                name = getattr(llm_obj, "model_name", None) or getattr(llm_obj, "model", None)
                base = getattr(llm_obj, "base_url", None)
                # LangChain æ–°ç‰ˆå¸¸æŠŠ client è—é‡Œé¢
                if base is None and hasattr(llm_obj, "client"):
                    base = getattr(llm_obj.client, "base_url", None)
            except Exception:
                pass
            # å…œåº•ï¼šrepr æˆªæ–­
            if name is None:
                try:
                    name = repr(llm_obj)[:60]
                except Exception:
                    name = "<unknown>"
            if base is None:
                base = os.getenv("OPENAI_API_BASE") or os.getenv("OPENAI_BASE_URL") or "<unknown>"
            return name, base

        # ç”¨æ›´ç¨³å¥çš„è¯†åˆ«ä¿¡æ¯æ‰“å°  [æ›¿æ¢ä½ åŸæ¥çš„ print(...)]
        try:
            _name, _base = _llm_ident(self.llm)
            print(f"[GenerationAgent] model={_name} base={_base} "
                  f"ctx={self.max_ctx_tokens} gen={self.max_gen_tokens}")
        except Exception:
            print(f"[GenerationAgent] ctx={self.max_ctx_tokens} gen={self.max_gen_tokens}")

    # ---- ç»¼åˆåˆ†ï¼ˆæŒ‰éœ€è°ƒæ•´æƒé‡ï¼‰----
    def _compute_combined_score(self, faith: float, relevancy: float, noise: float) -> float:
        # é˜²å¾¡ï¼šç©ºå€¼/è¶Šç•Œ
        f = float(faith or 0.0)
        r = float(relevancy or 0.0)
        n = float(noise if noise is not None else 1.0)
        n = min(max(n, 0.0), 1.0)
        return 0.65 * f + 0.25 * r + 0.10 * (1.0 - n)

    def _safe_semantic_f1(self, gold: str, pred: str) -> float:
        """
        ä¼˜å…ˆè°ƒç”¨ self.semantic_f1_metricï¼ˆè‹¥å¯ç”¨ï¼‰ï¼Œå¤±è´¥åˆ™å›é€€åˆ°ç¨³å¥çš„å­—ç¬¦ä¸²çº§è¯­ä¹‰ F1ï¼š
        - NFKD æŠ˜å  + å»é‡éŸ³ï¼ˆLiÃ¨ge -> liegeï¼‰
        - ç»Ÿä¸€å°å†™ã€å»æ ‡ç‚¹ã€åˆå¹¶ç©ºç™½
        - æç®€åœç”¨è¯å‰”é™¤ï¼ˆä¸è¯¯æ€åè¯ï¼‰
        - å¤šç­”æ¡ˆåˆ‡åˆ†ï¼ˆé€—å·/åˆ†å·/æ–œæ /or/æˆ–ï¼‰
        - â€œåŒ…å«å³æ»¡åˆ†â€çš„å¿«æ·é€šé“ï¼ˆå½’ä¸€åŒ–åå­ä¸²åŒ…å«ï¼‰
        """
        # ---------- åˆ¤å®šæ‹’ç­” ----------
        def _looks_like_refusal(t: str) -> bool:
            if not t: return False
            s = t.strip().lower()
            return any(kw in s for kw in [
                "not enough information", "cannot answer", "can't answer",
                "insufficient context", "unknown", "æŠ±æ­‰", "æ— æ³•", "æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯"
            ])

        # ---------- å½’ä¸€åŒ– ----------
        def _normalize_text(s: str) -> str:
            import re, unicodedata
            t = (s or "").strip().lower()
            # ç»Ÿä¸€å¼•å·/ç ´æŠ˜å·
            t = (t.replace("â€œ", "\"").replace("â€", "\"")
                .replace("â€™", "'").replace("â€˜", "'")
                .replace("â€”", "-"))
            # NFKD æŠ˜å  + å»é‡éŸ³ï¼ˆLiÃ¨ge -> liegeï¼‰
            t = unicodedata.normalize("NFKD", t)
            t = "".join(ch for ch in t if not unicodedata.combining(ch))
            # éå­—æ¯æ•°å­—è½¬ç©ºæ ¼
            t = re.sub(r"[^a-z0-9\s]", " ", t)
            # åˆå¹¶ç©ºç™½
            t = re.sub(r"\s+", " ", t).strip()
            return t

        # ---------- åˆ«åè¡¨ï¼ˆå¯æŒ‰éœ€æ‰©å……ï¼‰ ----------
        # æ”¾åœ¨ normalize åã€token å‰åšè¯çº§æ›¿æ¢æ›´ç®€å•ï¼›è¿™é‡Œåœ¨ token é›†ä¸Šåšç»Ÿä¸€åŒ–
        _ALIASES = {
            "deaflympic games": "deaflympics",
            "summer deaflympics": "deaflympics",
            "battle of tannenberg": "tannenberg",
            "battle of liege": "liege",
        }

        _STOP = {"the","of","and","a","an","to","in","on","for","with","at","by","from"}

        def _token_set(s: str) -> set:
            # å½’ä¸€åŒ– â†’ è¯è¡¨ â†’ åˆ«åæ”¶æ•› â†’ å»æç®€åœç”¨è¯
            norm = _normalize_text(s)
            toks = [w for w in norm.split() if w]
            # åˆ«åæ”¶æ•›ï¼ˆå•è¯æˆ–åŒè¯çŸ­è¯­åœ¨æ­¤å¤„å·²è¢«ç©ºç™½åˆ‡å¼€ï¼Œç®€å•æ”¶æ•›å•è¯çº§ï¼‰
            mapped = []
            i = 0
            while i < len(toks):
                # å°è¯•åŒè¯åˆ«å
                if i + 1 < len(toks):
                    two = f"{toks[i]} {toks[i+1]}"
                    if two in _ALIASES:
                        mapped.append(_ALIASES[two])
                        i += 2
                        continue
                w = toks[i]
                mapped.append(_ALIASES.get(w, w))
                i += 1
            return {w for w in mapped if w not in _STOP}

        def _contains_normed(needle: str, hay: str) -> bool:
            n = _normalize_text(needle)
            h = _normalize_text(hay)
            return bool(n) and (n in h)

        # ---------- å¤šç­”æ¡ˆåˆ‡åˆ† ----------
        def _split_multi(s: str) -> list:
            import re
            if not s: return []
            parts = re.split(r"\s*(?:,|;|/|\bor\b|æˆ–)\s*", str(s), flags=re.I)
            return [p for p in parts if p.strip()]

        # ---------- é›†åˆ F1ï¼ˆç¨³å¥ï¼‰ ----------
        def _set_f1(g: str, p: str) -> float:
            import re
            gs, ps = _token_set(g), _token_set(p)
            if not gs or not ps:
                return 0.0

            inter = len(gs & ps)
            union = len(gs | ps)
            prec = inter / len(ps)
            rec  = inter / len(gs)
            base = 0.0 if (prec + rec) == 0 else (2 * prec * rec) / (prec + rec)

            # ---- è½¯æ€§â€œåŒ…å«â€åŠ åˆ†ï¼Œè€Œä¸æ˜¯ç›´æ¥æ»¡åˆ† ----
            # 1) å½’ä¸€åŒ–åæ˜¯å¦æœ‰å­ä¸²å…³ç³»
            contains = _contains_normed(g, p) or _contains_normed(p, g)

            # 2) è¯çº§ç›¸ä¼¼åº¦ï¼ˆJaccardï¼‰åšä¸‹é™
            jaccard = inter / max(1, union)

            # 3) æ•°å­—ä¸€è‡´æ€§ï¼šå¦‚æœç­”æ¡ˆæ¶‰åŠå¹´ä»½/ç¼–å·ï¼Œå¼ºçº¦æŸæ›´å¯é 
            nums_g = set(re.findall(r"\d+", _normalize_text(g)))
            nums_p = set(re.findall(r"\d+", _normalize_text(p)))
            nums_ok = (not nums_g) or (nums_g <= nums_p)  # gold é‡Œçš„æ•°å­—éƒ½åœ¨ pred é‡Œ

            # è§„åˆ™ï¼šåŒ…å« ä¸” æ•°å­—ä¸€è‡´ æ—¶ï¼Œè‡³å°‘æ‹‰åˆ° 0.85ï¼›å¦åˆ™å¦‚æœ Jaccard>=0.5ï¼Œè‡³å°‘ 0.8
            if contains and nums_ok:
                base = max(base, 0.85)
            elif jaccard >= 0.5:
                base = max(base, 0.80)

            return min(base, 1.0)


        def _best_set_f1(gold_s: str, pred_s: str) -> float:
            gold_list = _split_multi(gold_s) or [gold_s]
            pred_list = _split_multi(pred_s) or [pred_s]
            best = 0.0
            for g in gold_list:
                for p in pred_list:
                    best = max(best, _set_f1(g, p))
                    if best == 1.0:
                        return 1.0
            return best

        # ---------- ä¸»æµç¨‹ ----------
        gold = "" if gold is None else str(gold).strip()
        pred = "" if pred is None else str(pred).strip()
        if not gold or not pred or _looks_like_refusal(pred):
            return 0.0

        # 1) ä¼˜å…ˆç”¨å¤–éƒ¨æŒ‡æ ‡ï¼ˆä¸ä½ ç°æœ‰é€»è¾‘ä¿æŒä¸€è‡´ï¼‰
        metric = getattr(self, "semantic_f1_metric", None)
        if callable(metric):
            # 1.1 ç›´æ¥å­—ç¬¦ä¸²è°ƒç”¨
            try:
                val = metric(gold, pred)
                try:
                    f = float(val)
                    if f == f:  # é NaN
                        return f
                except Exception:
                    pass
                if isinstance(val, dict):
                    for k in ("f1", "score", "semantic_f1"):
                        if k in val:
                            return float(val[k])
                for attr in ("score", "f1", "value"):
                    if hasattr(val, attr):
                        return float(getattr(val, attr))
            except Exception:
                pass
            # 1.2 dspy.Example é£æ ¼ï¼ˆå­˜åœ¨æ‰ç”¨ï¼‰
            try:
                import importlib
                dspy_mod = importlib.import_module("dspy")
                Example = getattr(dspy_mod, "Example", None)
            except Exception:
                Example = None
            if Example is not None:
                for fld in ("answer", "output", "response", "prediction"):
                    try:
                        e1, e2 = Example(**{fld: gold}), Example(**{fld: pred})
                        val = metric(e1, e2)
                        try:
                            f = float(val)
                            if f == f:
                                return f
                        except Exception:
                            if isinstance(val, dict):
                                for k in ("f1", "score", "semantic_f1"):
                                    if k in val:
                                        return float(val[k])
                            for attr in ("score", "f1", "value"):
                                if hasattr(val, attr):
                                    return float(getattr(val, attr))
                    except Exception:
                        pass

        # 3ï¸âƒ£ fallbackï¼šå­—ç¬¦ä¸² token F1
        val = _best_set_f1(gold, pred)
        print(f"[semF1.fallback] F1={val:.4f}  gold={gold[:50]!r}  pred={pred[:50]!r}")
        return val


    # ---- ä¸Šä¸‹æ–‡æ‹¼æ¥ + æˆªæ–­ ----
    def _trim_context(self, docs: List[Any], max_tokens: Optional[int] = None) -> str:
        max_tokens = max_tokens or self.max_ctx_tokens
        parts: List[str] = []
        for i, d in enumerate(docs, start=1):
            txt = _get_doc_text(d)
            if txt:
                parts.append(f"<Document {i}> {txt}")
        combined = "\n".join(parts)
        # è¿™é‡Œçš„ model ååªå½±å“ä¼°ç®—åˆ†è¯å™¨é€‰æ‹©ï¼›ä¸ä½ ç”¨çš„å®é™…ç”Ÿæˆæ¨¡å‹å¯ä»¥ä¸åŒ
        return trim_text_to_token_limit(combined, max_tokens=max_tokens, model="gpt-3.5-turbo")

    # ---- ç”Ÿæˆæç¤º ----
    def _build_prompt(self, question: str, context: str, attempt: int) -> str:
        instructions = """
You are a precise QA system. Answer questions using ONLY the retrieved context below.

RULES:
1. First, write a brief REASONING (1-3 sentences) that connects facts from the documents to derive the answer. Cite <Document N> when referencing information.
2. Then, write your FINAL ANSWER on a new line starting with "Answer:". Give the shortest possible answer: a name, date, number, place, or short phrase.
3. EVERY claim in your reasoning must come directly from the provided documents. Do NOT use external knowledge.
4. If the context does not contain enough information, respond with: Answer: insufficient context

Format:
Reasoning: <your brief chain of thought>
Answer: <shortest factual answer>
""".strip()
        if attempt > 0:
            instructions += (
                f"\n(Attempt #{attempt + 1}: Re-read the documents carefully. "
                "Make sure your answer is directly supported by the context.)"
            )

        prompt = f"""
{instructions}

Question: {question}

Context:
{context}

Answer (use Reasoning/Answer format):
""".strip()

        return safe_trim_prompt(prompt, model="gpt-3.5-turbo")

    # ---- CoT ç­”æ¡ˆè§£æ ----
    def _parse_cot_answer(self, raw_response: str) -> str:
        """ä» Reasoning/Answer æ ¼å¼ä¸­æå– Answer è¡Œã€‚"""
        for line in raw_response.strip().splitlines():
            stripped = line.strip()
            if stripped.lower().startswith("answer:"):
                return stripped[len("answer:"):].strip()
        return raw_response.strip()

    # ---- ç­”æ¡ˆå‹ç¼©ï¼ˆåå¤„ç†ï¼‰----
    def _extract_concise_answer(self, question: str, verbose_answer: str) -> str:
        """
        ç”¨ LLM ä»å†—é•¿å›ç­”ä¸­æå–æœ€ç²¾ç®€çš„æ ¸å¿ƒç­”æ¡ˆã€‚
        è‹¥å¤±è´¥åˆ™è¿”å›åŸç­”æ¡ˆã€‚
        """
        if not verbose_answer or len(verbose_answer.split()) <= 5:
            return verbose_answer  # å·²ç»è¶³å¤ŸçŸ­

        extract_prompt = (
            "Extract ONLY the core factual answer from the response below. "
            "Return just the entity name, date, number, or shortest phrase that "
            "directly answers the question. No explanation, no full sentences.\n\n"
            f"Question: {question}\n"
            f"Response: {verbose_answer}\n\n"
            "Core answer:"
        )
        try:
            msg = self.llm.invoke(extract_prompt)
            extracted = (getattr(msg, "content", "") or str(msg)).strip()
            # åªç”¨æŠ½å–ç»“æœå¦‚æœå®ƒæ¯”åŸç­”æ¡ˆæ›´çŸ­ä¸”éç©º
            if extracted and len(extracted) < len(verbose_answer) * 0.8:
                print(f"[AnswerExtract] '{verbose_answer[:60]}' â†’ '{extracted}'")
                return extracted
        except Exception:
            pass
        return verbose_answer

    # ---- ä¸»æµç¨‹ ----

    def answer(
        self,
        question: str,
        docs: List[Any],
        evaluation_agent,
        ground_truth: Optional[str] = None,
        max_attempts: int = 2,
        prompt_id: str = "gen_v1"
    ) -> Dict[str, Any]:
        """
        å¤šæ¬¡å°è¯• â†’ è¯„ä¼° â†’ æ—©åœï¼›åœ¨è¯„æµ‹å¤±æ•ˆ/é»˜è®¤å€¼æ—¶ä¹Ÿä¼šåˆ·æ–° best_answerï¼Œé¿å…è¿”å›ç©ºä¸²ã€‚
        """
        # ---- ä¸Šä¸‹æ–‡å…œåº• ----
        context = self._trim_context(docs) if docs else "<NO_RETRIEVED_CONTEXT>"

        # ==== A1. å…ˆè¯„ä¼°ä¸€æ¬¡æ£€ç´¢ï¼Œæ‹¿ precision / recall_likeï¼ˆä¼˜å…ˆçœŸ recallï¼Œå¦åˆ™ weak_recallï¼‰ ====
        try:
            retrieval_res = evaluation_agent.evaluate_retrieval(
                user_query=question, retrieved_docs=docs, reference=ground_truth
            ) or {}
        except Exception:
            retrieval_res = {}

        retr_prec = float(retrieval_res.get("context_precision", 0.0) or 0.0)
        retr_rec  = retrieval_res.get("context_recall", None)
        weak_rec  = retrieval_res.get("weak_recall", None)
        recall_like = float((retr_rec if retr_rec is not None else (weak_rec if weak_rec is not None else 0.0)) or 0.0)

        # ---- æ—¥å¿—å¢å¼ºï¼ˆEï¼‰----
        if self.logger:
            ctx_dbg = [ (getattr(d, "page_content", "") or "")[:120] for d in (docs or []) ][:2]
            self.logger.add_reason(
                f"[retr.dbg] k={len(docs or [])} prec={retr_prec:.2f} rec_like={recall_like:.2f} "
                f"ctx0={ctx_dbg[0] if ctx_dbg else ''}"
            )

        best_answer = ""
        best_combined_score = -1.0  # åˆå§‹å¾ˆä½ï¼Œä¾¿äºç¬¬ä¸€æ¬¡æœ‰æ•ˆç­”æ¡ˆè¦†ç›–
        best_metrics = {
            "faithfulness_score": 0.0,
            "response_relevancy": 0.0,
            "answer_relevancy": 0.0,   # é•œåƒï¼Œæ–¹ä¾¿ä¸‹æ¸¸è¯»å–
            "noise_sensitivity": 1.0,
            "semantic_f1_score": 0.0
        }
        best_eval_result = None
        best_latency_ms = 0.0

        # è®°å½•è¢«ç”¨åˆ°çš„æ–‡æ¡£ï¼ˆhash æ‘˜è¦ç”± logger è´Ÿè´£ï¼‰
        if self.logger:
            for d in docs or []:
                snippet = getattr(d, "page_content", "")[:200]
                if snippet:
                    self.logger.add_observation(snippet, do_hash=True)

        # ==== C. è¯­ä¹‰ F1 çš„å®‰å…¨è®¡ç®—ï¼ˆæ‹’ç­”â†’0ï¼›å­—æ®µå¯¹ç§°ï¼‰ ====



        for attempt in range(max_attempts):
            prompt = self._build_prompt(question, context, attempt)

            # ---- ç”Ÿæˆï¼ˆå¥å£®æŠ½å– + éç©ºå…œåº•ï¼‰----
            try:
                t0 = time.time()
                msg = self.llm.invoke(prompt)         # LangChain ChatOpenAI -> AIMessage
                gen_latency_ms = (time.time() - t0) * 1000.0

                answer_text = None
                # å¸¸è§„
                if hasattr(msg, "content"):
                    answer_text = msg.content
                # å…¼å®¹å…¶å®ƒå­—æ®µ
                if (not answer_text) and hasattr(msg, "message"):
                    answer_text = getattr(msg, "message")
                if (not answer_text) and hasattr(msg, "text"):
                    answer_text = getattr(msg, "text")
                # å…œåº•åˆ°å­—ç¬¦ä¸²
                if not answer_text:
                    answer_text = str(msg)

                # å¯èƒ½æ˜¯ list/å—ç»“æ„
                if isinstance(answer_text, (list, tuple)):
                    parts = []
                    for p in answer_text:
                        if isinstance(p, str):
                            parts.append(p)
                        elif isinstance(p, dict):
                            if "text" in p and isinstance(p["text"], str):
                                parts.append(p["text"])
                            elif "content" in p and isinstance(p["content"], str):
                                parts.append(p["content"])
                    answer_text = " ".join(parts).strip()

                answer_text = (answer_text or "").strip()
                if not answer_text:
                    answer_text = "[NO_ANSWER_GENERATED]"
            except Exception as e:
                gen_latency_ms = 0.0
                answer_text = "[NO_ANSWER_GENERATED]"
                if self.logger:
                    self.logger.add_reason(f"[gen.error] {type(e).__name__}: {e}")

            # ---- ç­”æ¡ˆè§£æ + å‹ç¼©ï¼ˆåå¤„ç†ï¼‰----
            if answer_text and answer_text != "[NO_ANSWER_GENERATED]":
                answer_text = self._parse_cot_answer(answer_text)
                answer_text = self._extract_concise_answer(question, answer_text)

            # ç”Ÿæˆæ—¥å¿—
            if self.logger:
                self.logger.add_generation(attempt=attempt + 1, prompt_id=prompt_id, answer=answer_text[:800])
                self.logger.add_eval(gen_latency_ms=round(gen_latency_ms, 2), attempt=attempt + 1)

            # ---- è¯„ä¼°ï¼ˆä¼  referenceï¼›æ‹¿åˆ†+statusï¼‰----
            try:
                eval_result = evaluation_agent.evaluate_generation(
                    user_query=question,
                    retrieved_docs=docs,
                    response=answer_text,
                    reference=ground_truth
                ) or {}
            except Exception as e:
                eval_result = {}
                if self.logger:
                    self.logger.add_reason(f"[eval.error] {type(e).__name__}: {e}")

            # åˆ†æ•°
            faithfulness_score = extract_scalar(eval_result.get("faithfulness", 0.0))
            relevancy_score_raw = eval_result.get("response_relevancy", None)
            if relevancy_score_raw is None:
                relevancy_score_raw = eval_result.get("answer_relevancy", 0.0)
            relevancy_score = extract_scalar(relevancy_score_raw)

            # statusï¼ˆé»˜è®¤missingï¼Œä¾¿äºåˆ¤æ–­ï¼‰
            faith_st = str(eval_result.get("faithfulness_status", "missing"))
            rel_st   = str(
                eval_result.get(
                    "response_relevancy_status",
                    eval_result.get("answer_relevancy_status", "missing")
                )
            )
            noise_st = "missing"

            # noise å…¼å®¹é”®å
            noise_sensitivity = None
            for k, v in eval_result.items():
                if "noise_sensitivity" in str(k):
                    noise_sensitivity = extract_scalar(v)
                    noise_st = str(
                        eval_result.get(f"{k}_status",
                        eval_result.get("noise_sensitivity_status", "ok"))
                    )
                    break
            if noise_sensitivity is None:
                noise_sensitivity, noise_st = 1.0, "missing"

            # ==== C. è¯­ä¹‰ F1ï¼ˆä¿®å¤ç‰ˆï¼‰ ====
            # å¯é€‰ï¼šè¯­ä¹‰ F1
            # === Semantic F1ï¼ˆç¨³å¥å…œåº•ç‰ˆï¼‰===  ğŸ‘‰ æ›¿æ¢ä½ åŸæ¥çš„â€œå¯é€‰ï¼šè¯­ä¹‰ F1â€æ•´æ®µ
            def _is_valid_score(x):
                try:
                    f = float(x)
                    return (f == f) and (0.0 <= f <= 1.0)   # é NaN ä¸”åœ¨ [0,1]
                except Exception:
                    return False

            semantic_f1_score = 0.0
            if ground_truth:
                used_metric = False
                f1_candidate = None
                metric = getattr(self, "semantic_f1_metric", None)

                # 1) ä¼˜å…ˆå¤–éƒ¨ metricï¼ˆä»…æ¥å—åˆæ³•å€¼ï¼‰
                if callable(metric):
                    try:
                        val = metric(ground_truth, answer_text)
                        if _is_valid_score(val):
                            used_metric, f1_candidate = True, float(val)
                        elif isinstance(val, dict):
                            for k in ("f1", "score", "semantic_f1"):
                                if k in val and _is_valid_score(val[k]):
                                    used_metric, f1_candidate = True, float(val[k])
                                    break
                        elif hasattr(val, "score") and _is_valid_score(getattr(val, "score")):
                            used_metric, f1_candidate = True, float(getattr(val, "score"))
                    except Exception:
                        used_metric = False

                    # 1b) è‹¥å¤–éƒ¨ metric æ”¯æŒ dspy.Example å½¢æ€ï¼Œå†è¯•ä¸€æ¬¡
                    if not used_metric:
                        try:
                            import importlib
                            dspy_mod = importlib.import_module("dspy")
                            Example = getattr(dspy_mod, "Example", None)
                            if Example is not None:
                                for fld in ("answer", "output", "response", "prediction"):
                                    try:
                                        e1, e2 = Example(**{fld: str(ground_truth)}), Example(**{fld: str(answer_text)})
                                        val = metric(e1, e2)
                                        if _is_valid_score(val):
                                            used_metric, f1_candidate = True, float(val)
                                            break
                                        if isinstance(val, dict):
                                            for k in ("f1", "score", "semantic_f1"):
                                                if k in val and _is_valid_score(val[k]):
                                                    used_metric, f1_candidate = True, float(val[k])
                                                    break
                                        if hasattr(val, "score") and _is_valid_score(getattr(val, "score")):
                                            used_metric, f1_candidate = True, float(getattr(val, "score"))
                                            break
                                    except Exception:
                                        pass
                        except Exception:
                            pass

                # 2) å…œåº•ï¼šèµ°ä½ è‡ªå·±çš„ _safe_semantic_f1ï¼ˆå¿…é¡»å­˜åœ¨äºç±»é‡Œï¼‰
                if not used_metric:
                    semantic_f1_score = self._safe_semantic_f1(str(ground_truth), str(answer_text))
                else:
                    semantic_f1_score = f1_candidate

                # ï¼ˆå¯é€‰ï¼‰åªåœ¨ 0 åˆ†æ—¶æ‰“å°ä¸€æ¬¡é›†åˆï¼Œä¾¿äºæ’æŸ¥å½’ä¸€åŒ–æ˜¯å¦æ´—æ‰äº†å…³é”®è¯ï¼š
                if semantic_f1_score == 0.0:
                    try:
                        import re, unicodedata
                        def _norm(s):
                            s = unicodedata.normalize("NFKD", str(s).lower().strip())
                            s = "".join(ch for ch in s if not unicodedata.combining(ch))
                            s = re.sub(r"[^a-z0-9\s]", " ", s)
                            return re.sub(r"\s+", " ", s).strip()
                        STOP = {"the","of","and","a","an","to","in","on","for","with","at","by","from"}
                        def _tokset(s): return {w for w in _norm(s).split() if w and w not in STOP}
                        print(f"[semF1.debug] ref_set={_tokset(ground_truth)} pred_set={_tokset(answer_text)}")
                    except Exception:
                        pass
            else:
                semantic_f1_score = 0.0



            # è¯„ä¼°æ—¥å¿—ï¼ˆä¾¿äºå¤–éƒ¨æ’æŸ¥ï¼‰
            if self.logger:
                self.logger.add_eval(
                    faith=faithfulness_score, response_relevancy=relevancy_score,
                    noise_sensitivity=noise_sensitivity, semantic_f1=semantic_f1_score,
                    faith_status=faith_st, relevancy_status=rel_st, noise_status=noise_st,
                    attempt=attempt + 1
                )
                # è°ƒè¯•æ‰“å°ä¸€è¡Œï¼ˆå¯ç•™å¯å»ï¼‰
                print(f"ğŸ”§ eval_result -> faith={faithfulness_score:.4f}({faith_st}), "
                    f"rel={relevancy_score:.4f}({rel_st}), noise={noise_sensitivity:.4f}({noise_st}), "
                    f"ans[:60]={answer_text[:60]!r}")

            # ---- æ—©åœ or åˆ·æ–°æœ€ä½³ ----
            combined_score = self._compute_combined_score(
                faithfulness_score, relevancy_score, noise_sensitivity,
            )

            # æ£€ç´¢å·® â†’ é™æƒ
            if retr_prec < 0.2 or recall_like < 0.2:
                combined_score *= 0.6

            # â€œæœ‰ç”¨åˆ†æ•°â€ï¼šä»»ä¸€ status ä¸º ok å³è§†ä¸ºæœ‰æ•ˆè¯„æµ‹
            has_any_valid = (faith_st == "ok") or (rel_st == "ok") or (noise_st == "ok")

            # ==== B. æ—©åœï¼ˆæ£€ç´¢åˆæ ¼ + æ¨¡å‹åˆ†è¾¾æ ‡ï¼‰ ====
            early_stop = (
                has_any_valid and
                (retr_prec >= 0.5 and recall_like >= 0.5) and
                faithfulness_score >= 0.8 and
                relevancy_score   >= 0.4 and
                noise_sensitivity <= 0.4 and
                semantic_f1_score >= 0.7
            )
            if early_stop:
                if self.logger:
                    self.logger.set_final_answer(answer_text)
                return {
                    "answer": answer_text,
                    "faithfulness_score": faithfulness_score,
                    "response_relevancy": relevancy_score,
                    "answer_relevancy":  relevancy_score,   # é•œåƒ
                    "noise_sensitivity": noise_sensitivity,
                    "semantic_f1_score": semantic_f1_score,
                    "cached_eval_result": eval_result,
                    "eval_result": eval_result,
                    "faith": faithfulness_score,
                    "semantic_f1": semantic_f1_score,
                    "latency_ms": gen_latency_ms
                }

            # åˆ·æ–°æœ€ä½³ï¼šâ‘  æœ‰ä»»ä½•æœ‰æ•ˆåˆ†ä¸”æ›´ä¼˜ï¼›â‘¡ æˆ–â€œå°šæœªå†™å…¥è¿‡ best_answerâ€ï¼ˆé˜²ç©ºä¸²ï¼‰
            if (has_any_valid and combined_score > best_combined_score) or (not best_answer.strip()):
                if has_any_valid:
                    best_combined_score = combined_score
                best_answer = answer_text
                best_metrics = {
                    "faithfulness_score": faithfulness_score,
                    "response_relevancy": relevancy_score,
                    "answer_relevancy": relevancy_score,  # é•œåƒï¼Œæ–¹ä¾¿ä¸‹æ¸¸è¯»å–
                    "noise_sensitivity": noise_sensitivity,
                    "semantic_f1_score": semantic_f1_score
                }
                best_eval_result = eval_result
                best_latency_ms = gen_latency_ms

        # ---- è¾¾æœ€å¤§æ¬¡æ•°ï¼Œè¿”å›æœ€ä½³ ----
        if self.logger:
            self.logger.set_final_answer(best_answer)

        return {
            "answer": best_answer,
            "faithfulness_score": best_metrics.get("faithfulness_score", 0.0),
            "response_relevancy": best_metrics.get("response_relevancy", 0.0),
            "answer_relevancy":   best_metrics.get("answer_relevancy", best_metrics.get("response_relevancy", 0.0)),
            "noise_sensitivity": best_metrics.get("noise_sensitivity", 1.0),
            "semantic_f1_score": best_metrics.get("semantic_f1_score", 0.0),
            "cached_eval_result": best_eval_result,
            "eval_result": best_eval_result,
            "faith": best_metrics.get("faithfulness_score", 0.0),
            "semantic_f1": best_metrics.get("semantic_f1_score", 0.0),
            "latency_ms": best_latency_ms
        }

import os
import json
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_core.documents import Document

def split_text_by_sentence(text, chunk_size=300, chunk_overlap=50):
    """åŸºäºå¥å­è¾¹ç•Œè¿›è¡Œæ–‡æœ¬åˆ‡åˆ†"""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        separators=["ã€‚", "ï¼Ÿ", "ï¼", "\n", ".", "?", "!"]  # æ”¯æŒä¸­è‹±æ–‡å¥å·
    )
    return text_splitter.split_text(text)

def load_hotpot_mini(json_path="data-hotpot/hotpot_mini_corpus.json"):
    """åŠ è½½Hotpot mini corpus (json)"""
    with open(json_path, 'r') as f:
        corpus = json.load(f)
    return corpus

def create_vectorstore(json_path="data-hotpot/hotpot_mini_corpus.json", persist_path="vectorstore-hotpot/hotpotqa_faiss"):
    """ä»Hotpot mini corpusåˆ›å»ºFAISSå‘é‡åº“"""

    # 1. åŠ è½½å°å‹corpus
    print("ğŸš€ Loading mini corpus...")
    corpus = load_hotpot_mini(json_path)
    print(f"âœ… Loaded {len(corpus)} items.")

    # 2. åšchunking
    print("âœ‚ï¸ Splitting into chunks...")
    docs = []
    for item in corpus:
        # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µåç§°: context è€Œä¸æ˜¯ content
        if "context" not in item:
            print(f"âš ï¸ Warning: Missing 'context' in item with question: {item.get('question', 'Unknown')}")
            continue

        context = item['context']
        question = item.get('question', '')
        answer = item.get('answer', '')

        # ä¸ºæ¯ä¸ªchunkæ·»åŠ é—®é¢˜å’Œç­”æ¡ˆä½œä¸ºå…ƒæ•°æ®ï¼Œä¾¿äºåç»­æ£€ç´¢
        chunks = split_text_by_sentence(context)
        for chunk in chunks:
            if chunk.strip():
                docs.append(Document(
                    page_content=chunk,
                    metadata={
                        "question": question,
                        "answer": answer,
                        # æˆ‘ä»¬å¯ä»¥ä»contextä¸­æå–æ ‡é¢˜ (å¦‚æœæ ¼å¼æ˜¯"Title: content")
                        "title": chunk.split(':', 1)[0] if ':' in chunk else ""
                    }
                ))

    print(f"âœ… Prepared {len(docs)} chunks from {len(corpus)} items.")

    # æ‰“å°ä¸€äº›ç¤ºä¾‹chunksä»¥ä¾¿éªŒè¯
    if docs:
        print("\nğŸ“ Sample chunks:")
        for i in range(min(3, len(docs))):
            print(f"Chunk {i+1}:")
            print(f"  Content: {docs[i].page_content[:100]}...")
            print(f"  Metadata: {docs[i].metadata}")
        print()

    # 3. å»ºç«‹Embeddingå’ŒVectorstore
    print("ğŸ§  Embedding documents...")
    embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")  # è·Ÿä½ mainé‡Œä¿æŒä¸€è‡´

    vectorstore = FAISS.from_documents(docs, embeddings)

    # 4. ä¿å­˜
    print(f"ğŸ’¾ Saving FAISS index to {persist_path}...")
    os.makedirs(persist_path, exist_ok=True)
    vectorstore.save_local(persist_path)

    print("âœ… Vectorstore created successfully.")
    print(f"  - Contains {len(docs)} vectors")
    print(f"  - Saved to: {persist_path}")

if __name__ == "__main__":
    create_vectorstore()

# main-hotpot.py

import os
from tqdm import tqdm
import numpy as np
from decimal import Decimal

# ====== Agents ======
from agents.reasoning_agent import ReasoningAgent
from agents.retrieval_agent import RetrievalAgent
from agents.evaluation_agent import EvaluationAgent
from agents.generation_agent import GenerationAgent

# ====== run_rag_pipeline å¯¼å…¥ï¼ˆä¼˜å…ˆæ ¹ç›®å½•ï¼Œå…¶æ¬¡ agents/ï¼‰======
try:
    from langgraph_rag import run_rag_pipeline  # é¡¹ç›®æ ¹ç›®å½•
except Exception:
    from agents.langgraph_rag import run_rag_pipeline  # å…¼å®¹æ—§è·¯å¾„

# ====== Vector & Embeddings ======
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings

# ====== LLMs / DSPy ======
import dspy
from dspy.evaluate import SemanticF1
from langchain_openai import ChatOpenAI


# =========================
# åŸºç¡€ç¯å¢ƒï¼ˆé»˜è®¤æœ¬åœ° llama.cpp å…¼å®¹ç«¯ç‚¹ï¼‰
# =========================
os.environ.setdefault("OPENAI_API_BASE", "http://127.0.0.1:8000/v1")
os.environ.setdefault("OPENAI_API_KEY", "EMPTY")  # llama.cpp ä¸æ ¡éªŒï¼Œä½†éœ€è¦å ä½
os.environ.setdefault("EVAL_MAX_TOKENS", "1024")     # åŸæ¥ 256 å¤ªå°ï¼Œå»ºè®® 1024~2048
os.environ.setdefault("DSPY_MAX_TOKENS", "384")      # dspy çš„ä¹Ÿç•¥å¢ä¸€ç‚¹ï¼ˆé¿å… Reasoning æˆªæ–­ï¼‰
os.environ.setdefault("GEN_MAX_GEN_TOKENS", "1024")   # ç”Ÿæˆ LLM çš„ä¹Ÿç•¥å¢ä¸€ç‚¹

# æ§åˆ¶é¡¹
USE_LOCAL_EMB = os.getenv("USE_LOCAL_EMB", "0") == "1"          # æœ¬åœ°åµŒå…¥ï¼ˆéœ€é‡å»ºç´¢å¼•ï¼‰
USE_ROUTER    = os.getenv("USE_ROUTER", "0") == "1"             # æ˜¯å¦å¯ç”¨ Router/LangGraph
TESTSET_SIZE  = int(os.getenv("TESTSET_SIZE", "5"))             # æµ‹è¯•æ ·æœ¬æ•°
EVAL_MODE     = os.getenv("EVAL_MODE", "hybrid")                # strict / lenient / hybrid

# api keys/base
OPENAI_API_BASE      = os.getenv("OPENAI_API_BASE", "http://127.0.0.1:8000/v1")
OPENAI_API_KEY       = os.getenv("OPENAI_API_KEY", "EMPTY")
OPENAI_API_KEY_REAL  = os.getenv("OPENAI_API_KEY_REAL")  # çœŸå® keyï¼ˆè‹¥ç”¨äº‘ç«¯ï¼‰


# ğŸ”§ æ ‡é‡æŠ½å–ï¼ˆå…¼å®¹ list/np/Decimalï¼‰
def extract_scalar(val):
    import numpy as _np
    if val is None:
        return 0.0
    # list / tuple / np array å–å¹³å‡æˆ–ç¬¬ä¸€ä¸ªï¼ˆä½ å–œæ¬¢å“ªä¸ªå°±å›ºå®šä¸€ä¸ªç­–ç•¥ï¼‰
    if isinstance(val, (list, tuple)):
        try:
            xs = [float(x) for x in val if x is not None]
            return float(sum(xs) / len(xs)) if xs else 0.0
        except Exception:
            return 0.0
    # numpy æ ‡é‡
    if isinstance(val, (_np.floating, _np.integer, _np.generic)):
        return float(val)
    # æ™®é€šæ•°å­—
    if isinstance(val, (int, float)):
        return float(val)
    # å¯è½¬ float çš„å­—ç¬¦ä¸²
    try:
        return float(val)
    except Exception:
        # å…¼å®¹ ragas MetricResult å¯¹è±¡ï¼šæœ‰ .score / .value
        s = getattr(val, "score", None)
        if s is not None:
            try: return float(s)
            except: pass
        v = getattr(val, "value", None)
        if v is not None:
            try: return float(v)
            except: pass
        # å…¼å®¹ dict
        if isinstance(val, dict):
            for k in ("score", "value", "mean", "avg"):
                if k in val:
                    try: return float(val[k])
                    except: pass
        return 0.0



def _final_metric(result: dict, name: str, default: float = 0.0) -> float:
    """
    ä¼˜å…ˆå– result['metrics'][name]ï¼ˆé€šå¸¸æ˜¯é‡è¯•åæœ€ç»ˆå€¼ï¼‰ï¼Œ
    æ²¡æœ‰å†å›é€€åˆ°é¡¶å±‚ result[name]ï¼Œæœ€åç»™é»˜è®¤å€¼ã€‚
    """
    m = (result.get("metrics") or {}).get(name, None)
    if m is None:
        m = result.get(name, None)
    return extract_scalar(m if m is not None else default)

def is_success(result: dict) -> bool:
    """ç»Ÿä¸€åˆ¤æ–­æ˜¯å¦é€šè¿‡ï¼ˆå®¹é”™ ragas æŒ‡æ ‡ç¼ºå¤±/è§£æå¤±è´¥ï¼‰"""
    mode   = EVAL_MODE.lower()

    faith  = extract_scalar(result.get("faithfulness_score", 0.0))
    rel    = extract_scalar(result.get("response_relevancy", 0.0))
    noise  = extract_scalar(result.get("noise_sensitivity", 1.0))
    sem_f1 = extract_scalar(result.get("semantic_f1_score", result.get("semantic_f1", 0.0)))
    # ç”¨æœ€ç»ˆå€¼ï¼ˆå¯èƒ½æ˜¯æ£€ç´¢é‡è¯•åçš„ï¼‰
    recall = _final_metric(result, "context_recall", 0.0)

    # è¯»å–çŠ¶æ€ï¼ˆè‹¥ evaluate_generation è¿”å›äº†è¿™äº›å­—æ®µï¼‰
    ev           = result.get("eval_result") or {}
    faith_st     = str(ev.get("faithfulness_status", "ok"))
    rel_st       = str(ev.get("response_relevancy_status", "ok"))
    noise_st     = str(ev.get("noise_sensitivity_status", "ok"))
    any_unreliable = (faith_st != "ok") or (rel_st != "ok") or (noise_st != "ok")

    # è§„åˆ™
    ruleA = (faith >= 0.7 and rel >= 0.7 and noise <= 0.4 and sem_f1 >= 0.7)
    ruleB = (sem_f1 >= 0.85 and recall >= 0.7)  # å›é€€ï¼šç­”æ¡ˆå¯¹+å¬å›å¤Ÿ

    if mode == "strict":
        passed = ruleA
    elif mode == "lenient":
        passed = (sem_f1 >= 0.75 and recall >= 0.7)
    else:  # hybrid
        passed = (ruleA or ruleB) if not any_unreliable else ruleB

    # å¯é€‰è°ƒè¯•æ‰“å°ï¼šç¡®ä¿ä½ çœ‹è§ç”¨æ¥åˆ¤å®šçš„â€œæœ€ç»ˆ recallâ€
    print(f"ğŸ§ª pass_check | ruleA={ruleA} ruleB={ruleB} any_unreliable={any_unreliable} | "
          f"F1={sem_f1:.2f} faith={faith:.2f} rel={rel:.2f} noise={noise:.2f} ctxR(final)={recall:.2f} ({mode})")
    return passed



def main():
    # ------------------------
    # (1) é…ç½® DSPy ä½¿ç”¨æœ¬åœ°/äº‘ç«¯ï¼ˆä¼˜å…ˆçœŸå® keyï¼›å¦åˆ™é€€åˆ°æœ¬åœ°ï¼‰
    # ------------------------
    dspy.configure(
        lm=dspy.LM(
            model=os.getenv("DSPY_MODEL", "gpt-3.5-turbo"),
            api_base=OPENAI_API_BASE,
            api_key=OPENAI_API_KEY_REAL or OPENAI_API_KEY or "EMPTY",
            temperature=0.0,
            top_p=1.0,
            max_tokens=int(os.getenv("DSPY_MAX_TOKENS", "256")),
            timeout=int(os.getenv("DSPY_TIMEOUT", "30")),
        )
    )

    print(f"ğŸ§­ USE_ROUTER={USE_ROUTER} | USE_LOCAL_EMB={USE_LOCAL_EMB} | EVAL_MODE={EVAL_MODE}")

    # ------------------------
    # (2) æ„å»ºè¯„ä¼°/ç”Ÿæˆç”¨çš„ LLM
    # ------------------------
    # ç”Ÿæˆ LLMï¼šæŒ‰ä½ çš„éœ€æ±‚ï¼Œé»˜è®¤èµ°æœ¬åœ°ç«¯ç‚¹ï¼ˆä¹Ÿå¯æ”¹æˆäº‘ç«¯ï¼‰
    gen_llm = ChatOpenAI(
        model=os.getenv("GEN_LLM_MODEL", "gpt-3.5-turbo"),
        base_url=OPENAI_API_BASE,
        api_key=OPENAI_API_KEY or "EMPTY",
        temperature=0.0,
        max_tokens=int(os.getenv("GEN_MAX_GEN_TOKENS", "256")),
        timeout=float(os.getenv("LC_TIMEOUT", "60")),
    )

    # è¯„ä¼° LLMï¼šè‹¥æœ‰çœŸå® keyï¼Œåˆ™èµ°â€œå®˜æ–¹äº‘ç«¯â€æ›´ç¨³å®šï¼›å¦åˆ™é€€åˆ°æœ¬åœ°ç«¯ç‚¹
    if OPENAI_API_KEY_REAL:
        eval_llm = ChatOpenAI(
            model=os.getenv("EVAL_LLM_MODEL", "gpt-3.5-turbo"),
            base_url="https://api.openai.com/v1",
            api_key=OPENAI_API_KEY_REAL,
            temperature=0.0,
            top_p=1.0,
            max_tokens=int(os.getenv("EVAL_MAX_TOKENS", "256")),
            timeout=float(os.getenv("EVAL_TIMEOUT", "60")),
            max_retries=int(os.getenv("EVAL_MAX_RETRIES", "0")),
        )
        print("ğŸ” Evaluation LLM: using OpenAI cloud endpoint.")
    else:
        eval_llm = ChatOpenAI(
            model=os.getenv("EVAL_LLM_MODEL", "gpt-3.5-turbo"),
            base_url=OPENAI_API_BASE,
            api_key=OPENAI_API_KEY or "EMPTY",
            temperature=0.0,
            top_p=1.0,
            max_tokens=int(os.getenv("EVAL_MAX_TOKENS", "1024")),
            timeout=float(os.getenv("EVAL_TIMEOUT", "90")),
            max_retries=int(os.getenv("EVAL_MAX_RETRIES", "0")),
        )
        print("ğŸ” Evaluation LLM: no real key found, falling back to local endpoint.")

    # ------------------------
    # (3) Embeddings & FAISS
    # ------------------------
    if USE_LOCAL_EMB:
        # âš ï¸ å¿…é¡»ç”¨è¯¥æ¨¡å‹**é‡å»º**å‘é‡åº“ï¼Œå¦åˆ™ä¼šç»´åº¦ä¸åŒ¹é…ï¼
        print("ğŸ§  Embeddings: HuggingFace (sentence-transformers/all-MiniLM-L6-v2, dim=384)")
        embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
        vectorstore_path = os.getenv("FAISS_PATH_LOCAL", "vectorstore-hotpot/hf-miniLM-faiss")
    else:
        print("ğŸ§  Embeddings: OpenAI (text-embedding-ada-002, dim=1536) â€” must match your existing FAISS index.")
        embeddings = OpenAIEmbeddings(
            model=os.getenv("EMB_MODEL", "text-embedding-ada-002"),
            api_key=OPENAI_API_KEY_REAL or os.getenv("OPENAI_API_KEY"),
            base_url="https://api.openai.com/v1",
        )
        vectorstore_path = os.getenv("FAISS_PATH_OPENAI", "vectorstore-hotpot/hotpotqa_faiss")

    vectorstore = FAISS.load_local(
        vectorstore_path,
        embeddings,
        allow_dangerous_deserialization=True
    )

    # ------------------------
    # (4) åˆå§‹åŒ–å„ Agent
    # ------------------------
    semantic_f1_metric = SemanticF1(decompositional=True)

    reasoning_agent = ReasoningAgent()                 # å†…éƒ¨å·²é…ç½® dspy
    evaluation_agent = EvaluationAgent(llm=eval_llm)   # è¯„ä¼°èµ° eval_llmï¼ˆäº‘ç«¯ä¼˜å…ˆï¼‰
    retrieval_agent = RetrievalAgent(
        vectorstore=vectorstore,
        evaluation_agent=evaluation_agent,
        top_k=int(os.getenv("RETR_TOP_K", "3"))
    )
    generation_agent = GenerationAgent(
        llm=gen_llm,
        semantic_f1_metric=semantic_f1_metric
    )

    # ------------------------
    # (5) å–æµ‹è¯•é›†å¹¶å¾ªç¯è¯„æµ‹
    # ------------------------
    # ReasoningAgent.load_dataset() åœ¨ __init__ é‡Œå·²è·‘ï¼›æ•°æ®é›†è·¯å¾„è¯·ä¿è¯å­˜åœ¨
    total = min(TESTSET_SIZE, len(reasoning_agent.testset))
    subset_testset = reasoning_agent.testset[:total]

    print("\nğŸ§ª Running test set evaluation...")
    correct = 0
    faithfulness_scores = []
    semantic_f1_scores = []
    failed_cases = []

    for i, example in enumerate(tqdm(subset_testset, desc="Evaluating test set")):
        question = example.question
        ground_truth = example.response
        print(f"\nğŸ” Test {i+1}/{total}\nQ: {question}")
        print(f"ğŸ“– Ground Truth: {ground_truth}")  # æ–°å¢è¿™ä¸€è¡Œ

        result = run_rag_pipeline(
            question=question,
            retrieval_agent=retrieval_agent,
            reasoning_agent=reasoning_agent,
            generation_agent=generation_agent,
            evaluation_agent=evaluation_agent,
            reference=ground_truth,   # ä¼ å…¥ GT ä»¥ä¾¿ ctxR / semanticF1
            visualize=False,
            use_router=USE_ROUTER
        )

        ctxR_used = _final_metric(result, "context_recall", 0.0)
        ctxP_used = _final_metric(result, "context_precision", 0.0)
        print(f"ğŸ” Used-for-pass (final): ctxR={ctxR_used:.4f} ctxP={ctxP_used:.4f}")

        predicted_answer   = result.get("answer", "")
        faithfulness_score = extract_scalar(result.get("faithfulness_score", 0.0))
        relevancy_score    = extract_scalar(result.get("response_relevancy", 0.0))
        noise_score        = extract_scalar(result.get("noise_sensitivity", 1.0))
        semantic_f1_score  = extract_scalar(result.get("semantic_f1_score", 0.0))

        faithfulness_scores.append(faithfulness_score)
        semantic_f1_scores.append(semantic_f1_score)

        if is_success(result):
            correct += 1
        else:
            failed_cases.append({
                "question": question,
                "ground_truth": ground_truth,
                "predicted_answer": predicted_answer,
                "faithfulness_score": faithfulness_score,
                "relevancy_score": relevancy_score,
                "noise_score": noise_score,
                "semantic_f1_score": semantic_f1_score,
                "context_recall": ctxR_used,       # æœ€ç»ˆ
                "context_precision": ctxP_used     # æœ€ç»ˆ
            })

    avg_faithfulness = sum(faithfulness_scores) / len(faithfulness_scores) if faithfulness_scores else 0.0
    avg_f1 = sum(semantic_f1_scores) / len(semantic_f1_scores) if semantic_f1_scores else 0.0
    accuracy = (correct / total * 100.0) if total > 0 else 0.0

    print(f"\nâœ… Test accuracy: {correct}/{total} ({accuracy:.2f}%)")
    print(f"ğŸ“Š Average faithfulness score: {avg_faithfulness:.2f}")
    print(f"ğŸ§® Average Semantic F1 score: {avg_f1:.2f}")

    if failed_cases:
        print("\nâš ï¸ Failed cases (head 5):")
        for case in failed_cases[:5]:
            print("\nğŸ” Question:", case["question"])
            print("ğŸ“– Standard answer:", case["ground_truth"])
            print("ğŸ“ Predicted answer:", case["predicted_answer"])
            print(f"ğŸ“Š Faithfulness: {case['faithfulness_score']:.2f}")
            print(f"ğŸ¯ Relevancy: {case['relevancy_score']:.2f}")
            print(f"ğŸ”Š Noise sensitivity: {case['noise_score']:.2f}")
            print(f"âœ… Semantic F1: {case['semantic_f1_score']:.2f}")


if __name__ == "__main__":
    main()

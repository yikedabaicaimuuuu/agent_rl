# agents/generation_agent.py
from typing import Optional, Dict, Any, List, Union
import os, time, sys
import numpy as np
import dspy
from langchain_openai import ChatOpenAI

# å…è®¸ä»é¡¹ç›®æ ¹å¯¼å…¥ utils
import os as _os
sys.path.append(_os.path.abspath(_os.path.join(_os.path.dirname(__file__), '..')))

# å·¥å…·
from utils.text_utils import safe_trim_prompt, trim_text_to_token_limit
from utils.trajectory_logger import TrajectoryLogger


NumberLike = Union[int, float, np.floating, np.generic, str, List[Any]]


def extract_scalar(val: NumberLike) -> float:
    """å°† ragas/è¯„ä¼°è¿”å›çš„å„ç§ç±»å‹ç¨³å¥è½¬æˆ floatã€‚"""
    if val is None:
        return 0.0
    # list: å–å‡å€¼ï¼ˆæ›´ç¨³ï¼‰ï¼Œå¦‚æœéœ€è¦å–é¦–å…ƒç´ æ”¹æˆ float(val[0])
    if isinstance(val, list):
        nums = []
        for v in val:
            try:
                nums.append(float(v))
            except Exception:
                pass
        return float(np.mean(nums)) if nums else 0.0
    # numpy / int / float / str
    try:
        return float(val)
    except Exception:
        # dict.value å…œåº•
        if hasattr(val, "value"):
            try:
                return float(getattr(val, "value"))
            except Exception:
                return 0.0
        return 0.0


def _get_doc_text(d) -> str:
    """ä»ä»»æ„ doc æŠ½æ–‡æœ¬å†…å®¹ã€‚"""
    if d is None:
        return ""
    txt = getattr(d, "page_content", None)
    if isinstance(txt, str):
        return txt
    if isinstance(d, dict):
        for k in ("page_content", "text", "content"):
            v = d.get(k)
            if isinstance(v, str):
                return v
    if isinstance(d, str):
        return d
    return ""


class GenerationAgent:
    def __init__(
        self,
        model_name: Optional[str] = None,
        llm: Optional[ChatOpenAI] = None,
        semantic_f1_metric=None,
        logger: Optional[TrajectoryLogger] = None,
        max_ctx_tokens: int = 1400,
        max_gen_tokens: int = 512
    ):
        """
        - äº‘ç«¯ä¼˜å…ˆï¼šOPENAI_API_KEY_REAL â†’ å®˜æ–¹ç«¯ç‚¹ï¼›å¦åˆ™å›è½æœ¬åœ° OPENAI_API_BASEã€‚
        - æ§åˆ¶ä¸Šä¸‹æ–‡/ç”Ÿæˆé•¿åº¦ï¼Œé€‚é…è½»é‡èµ„æºã€‚
        """
        self.logger = logger
        self.semantic_f1_metric = semantic_f1_metric

        # è½»é‡æ¨¡å¼å¯é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–
        LIGHT_MODE = os.getenv("LIGHT_MODE", "0") == "1"
        if LIGHT_MODE:
            max_ctx_tokens = int(os.getenv("GEN_MAX_CTX_TOKENS", max_ctx_tokens))
            max_gen_tokens = int(os.getenv("GEN_MAX_GEN_TOKENS", max_gen_tokens))
        self.max_ctx_tokens = max_ctx_tokens
        self.max_gen_tokens = max_gen_tokens

        if llm is not None:
            self.llm = llm
        else:
            # äº‘ç«¯ä¼˜å…ˆ
            real_key = os.getenv("OPENAI_API_KEY_REAL")
            if real_key:
                base_url = "https://api.openai.com/v1"
                api_key = real_key
            else:
                base_url = os.getenv("OPENAI_API_BASE", "http://127.0.0.1:8000/v1")
                api_key = os.getenv("OPENAI_API_KEY", "sk-fake")

            default_model = "gpt-3.5-turbo"
            model_name = model_name or os.getenv("GEN_LLM_MODEL", default_model)

            self.llm = ChatOpenAI(
                model=model_name,                 # æ–°ç‰ˆç”¨ model=
                base_url=base_url,
                api_key=api_key,
                temperature=0.0,
                max_tokens=self.max_gen_tokens,
                top_p=1.0,
                max_retries=int(os.getenv("LC_MAX_RETRIES", "1")),
                timeout=float(os.getenv("LC_TIMEOUT", "60")),
            )

        # === æ›´ç¨³å¥çš„ LLM æ ‡è¯†ï¼ˆé¿å… Noneï¼‰ ===  [æ–°å¢]
        def _llm_ident(llm_obj):
            name = None
            base = None
            try:
                # å¸¸è§å­—æ®µ
                name = getattr(llm_obj, "model_name", None) or getattr(llm_obj, "model", None)
                base = getattr(llm_obj, "base_url", None)
                # LangChain æ–°ç‰ˆå¸¸æŠŠ client è—é‡Œé¢
                if base is None and hasattr(llm_obj, "client"):
                    base = getattr(llm_obj.client, "base_url", None)
            except Exception:
                pass
            # å…œåº•ï¼šrepr æˆªæ–­
            if name is None:
                try:
                    name = repr(llm_obj)[:60]
                except Exception:
                    name = "<unknown>"
            if base is None:
                base = os.getenv("OPENAI_API_BASE") or os.getenv("OPENAI_BASE_URL") or "<unknown>"
            return name, base

        # ç”¨æ›´ç¨³å¥çš„è¯†åˆ«ä¿¡æ¯æ‰“å°  [æ›¿æ¢ä½ åŸæ¥çš„ print(...)]
        try:
            _name, _base = _llm_ident(self.llm)
            print(f"[GenerationAgent] model={_name} base={_base} "
                  f"ctx={self.max_ctx_tokens} gen={self.max_gen_tokens}")
        except Exception:
            print(f"[GenerationAgent] ctx={self.max_ctx_tokens} gen={self.max_gen_tokens}")

    # ---- ç»¼åˆåˆ†ï¼ˆæŒ‰éœ€è°ƒæ•´æƒé‡ï¼‰----
    def _compute_combined_score(self, faith: float, relevancy: float, noise: float) -> float:
        # é˜²å¾¡ï¼šç©ºå€¼/è¶Šç•Œ
        f = float(faith or 0.0)
        r = float(relevancy or 0.0)
        n = float(noise if noise is not None else 1.0)
        n = min(max(n, 0.0), 1.0)
        return 0.65 * f + 0.25 * r + 0.10 * (1.0 - n)

    def _safe_semantic_f1(self, gold: str, pred: str) -> float:
        """
        ä¼˜å…ˆè°ƒç”¨ self.semantic_f1_metricï¼ˆè‹¥å¯ç”¨ï¼‰ï¼Œå¤±è´¥åˆ™å›é€€åˆ°å­—ç¬¦ä¸²çº§ token F1ã€‚
        æ”¯æŒå¤šç­”æ¡ˆåˆ‡åˆ†ï¼›æ‹’ç­”çŸ­è¯­â†’0 åˆ†ã€‚
        """
        def _looks_like_refusal(t: str) -> bool:
            if not t: return False
            s = t.strip().lower()
            return any(kw in s for kw in [
                "not enough information", "cannot answer", "can't answer",
                "insufficient context", "unknown", "æŠ±æ­‰", "æ— æ³•", "æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯"
            ])

        def _normalize_text(s: str) -> str:
            import re
            t = (s or "").strip().lower()
            t = (t.replace("â€œ","\"").replace("â€","\"")
                .replace("â€™","'").replace("â€˜","'")
                .replace("â€”","-"))
            t = re.sub(r"[^\w\s]", " ", t)
            t = re.sub(r"\s+", " ", t).strip()
            return t

        def _split_multi(s: str) -> list:
            import re
            if not s: return []
            parts = re.split(r"\s*(?:,|;|/|\bor\b|æˆ–)\s*", str(s), flags=re.I)
            return [p for p in parts if p.strip()]

        def _token_f1(g: str, p: str) -> float:
            from collections import Counter
            g_norm, p_norm = _normalize_text(g), _normalize_text(p)
            if not g_norm or not p_norm:
                return 0.0
            g_toks, p_toks = g_norm.split(), p_norm.split()
            if not g_toks or not p_toks:
                return 0.0
            g_c, p_c = Counter(g_toks), Counter(p_toks)
            overlap = sum((g_c & p_c).values())
            if overlap <= 0:
                return 0.0
            precision = overlap / len(p_toks)
            recall    = overlap / len(g_toks)
            return 0.0 if (precision + recall) == 0 else 2 * precision * recall / (precision + recall)

        def _best_token_f1(gold_s: str, pred_s: str) -> float:
            gold_list = _split_multi(gold_s) or [gold_s]
            pred_list = _split_multi(pred_s) or [pred_s]
            best = 0.0
            for g in gold_list:
                for p in pred_list:
                    g_norm, p_norm = _normalize_text(g), _normalize_text(p)
                    if g_norm and (g_norm in p_norm or p_norm in g_norm):
                        return 1.0
                    best = max(best, _token_f1(g, p))
            return best

        gold = "" if gold is None else str(gold).strip()
        pred = "" if pred is None else str(pred).strip()
        if not gold or not pred:
            return 0.0
        if _looks_like_refusal(pred):
            return 0.0

        metric = getattr(self, "semantic_f1_metric", None)
        if callable(metric):
            # 1) ç›´æ¥å­—ç¬¦ä¸²
            try:
                val = metric(gold, pred)
                try:
                    return float(val)
                except Exception:
                    pass
                if isinstance(val, dict):
                    for k in ("f1", "score", "semantic_f1"):
                        if k in val:
                            return float(val[k])
                for attr in ("score", "f1", "value"):
                    if hasattr(val, attr):
                        return float(getattr(val, attr))
            except Exception:
                pass
            # 2) Example é£æ ¼ï¼ˆæƒ°æ€§å¯¼å…¥ï¼Œç¯å¢ƒæ—  dspy ä¹Ÿä¸ä¼šæŠ¥é”™ï¼‰
            try:
                import importlib
                dspy_mod = importlib.import_module("dspy")
                Example = getattr(dspy_mod, "Example", None)
            except Exception:
                Example = None
            if Example is not None:
                for fld in ("answer", "output", "response", "prediction"):
                    try:
                        e1, e2 = Example(**{fld: gold}), Example(**{fld: pred})
                        val = metric(e1, e2)
                        try:
                            return float(val)
                        except Exception:
                            if isinstance(val, dict):
                                for k in ("f1", "score", "semantic_f1"):
                                    if k in val:
                                        return float(val[k])
                            for attr in ("score", "f1", "value"):
                                if hasattr(val, attr):
                                    return float(getattr(val, attr))
                    except Exception:
                        pass

        # 3) å…œåº•ï¼šå­—ç¬¦ä¸² token F1
        return _best_token_f1(gold, pred)


    # ---- ä¸Šä¸‹æ–‡æ‹¼æ¥ + æˆªæ–­ ----
    def _trim_context(self, docs: List[Any], max_tokens: Optional[int] = None) -> str:
        max_tokens = max_tokens or self.max_ctx_tokens
        parts: List[str] = []
        for i, d in enumerate(docs, start=1):
            txt = _get_doc_text(d)
            if txt:
                parts.append(f"<Document {i}> {txt}")
        combined = "\n".join(parts)
        # è¿™é‡Œçš„ model ååªå½±å“ä¼°ç®—åˆ†è¯å™¨é€‰æ‹©ï¼›ä¸ä½ ç”¨çš„å®é™…ç”Ÿæˆæ¨¡å‹å¯ä»¥ä¸åŒ
        return trim_text_to_token_limit(combined, max_tokens=max_tokens, model="gpt-3.5-turbo")

    # ---- ç”Ÿæˆæç¤º ----
    def _build_prompt(self, question: str, context: str, attempt: int) -> str:
        instructions = """
You are an AI assistant that answers questions strictly based on the retrieved documents.

Instructions:
1) Extract key facts from the retrieved context.
2) Summarize the points that directly answer the question.
3) Write the final answer explicitly and concisely, mirroring key entities in the question.
4) If the context is insufficient, say: "The provided context does not contain enough information to answer this question."
Do NOT mention the retrieval process or speculate beyond the context.
""".strip()
        if attempt > 0:
            instructions += f"\n(Note: This is attempt #{attempt + 1}. Improve faithfulness and concision.)"

        prompt = f"""
{instructions}

Question:
{question}

Retrieved Context:
{context}

Answer strictly based on the retrieved context above:
""".strip()

        return safe_trim_prompt(prompt, model="gpt-3.5-turbo")

    # ---- ä¸»æµç¨‹ ----

    def answer(
        self,
        question: str,
        docs: List[Any],
        evaluation_agent,
        ground_truth: Optional[str] = None,
        max_attempts: int = 2,
        prompt_id: str = "gen_v1"
    ) -> Dict[str, Any]:
        """
        å¤šæ¬¡å°è¯• â†’ è¯„ä¼° â†’ æ—©åœï¼›åœ¨è¯„æµ‹å¤±æ•ˆ/é»˜è®¤å€¼æ—¶ä¹Ÿä¼šåˆ·æ–° best_answerï¼Œé¿å…è¿”å›ç©ºä¸²ã€‚
        """
        # ---- ä¸Šä¸‹æ–‡å…œåº• ----
        context = self._trim_context(docs) if docs else "<NO_RETRIEVED_CONTEXT>"

        # ==== A1. å…ˆè¯„ä¼°ä¸€æ¬¡æ£€ç´¢ï¼Œæ‹¿ precision / recall_likeï¼ˆä¼˜å…ˆçœŸ recallï¼Œå¦åˆ™ weak_recallï¼‰ ====
        try:
            retrieval_res = evaluation_agent.evaluate_retrieval(
                user_query=question, retrieved_docs=docs, reference=ground_truth
            ) or {}
        except Exception:
            retrieval_res = {}

        retr_prec = float(retrieval_res.get("context_precision", 0.0) or 0.0)
        retr_rec  = retrieval_res.get("context_recall", None)
        weak_rec  = retrieval_res.get("weak_recall", None)
        recall_like = float((retr_rec if retr_rec is not None else (weak_rec if weak_rec is not None else 0.0)) or 0.0)

        # ---- æ—¥å¿—å¢å¼ºï¼ˆEï¼‰----
        if self.logger:
            ctx_dbg = [ (getattr(d, "page_content", "") or "")[:120] for d in (docs or []) ][:2]
            self.logger.add_reason(
                f"[retr.dbg] k={len(docs or [])} prec={retr_prec:.2f} rec_like={recall_like:.2f} "
                f"ctx0={ctx_dbg[0] if ctx_dbg else ''}"
            )

        best_answer = ""
        best_combined_score = -1.0  # åˆå§‹å¾ˆä½ï¼Œä¾¿äºç¬¬ä¸€æ¬¡æœ‰æ•ˆç­”æ¡ˆè¦†ç›–
        best_metrics = {
            "faithfulness_score": 0.0,
            "response_relevancy": 0.0,
            "answer_relevancy": 0.0,   # é•œåƒï¼Œæ–¹ä¾¿ä¸‹æ¸¸è¯»å–
            "noise_sensitivity": 1.0,
            "semantic_f1_score": 0.0
        }
        best_eval_result = None
        best_latency_ms = 0.0

        # è®°å½•è¢«ç”¨åˆ°çš„æ–‡æ¡£ï¼ˆhash æ‘˜è¦ç”± logger è´Ÿè´£ï¼‰
        if self.logger:
            for d in docs or []:
                snippet = getattr(d, "page_content", "")[:200]
                if snippet:
                    self.logger.add_observation(snippet, do_hash=True)

        # ==== C. è¯­ä¹‰ F1 çš„å®‰å…¨è®¡ç®—ï¼ˆæ‹’ç­”â†’0ï¼›å­—æ®µå¯¹ç§°ï¼‰ ====



        for attempt in range(max_attempts):
            prompt = self._build_prompt(question, context, attempt)

            # ---- ç”Ÿæˆï¼ˆå¥å£®æŠ½å– + éç©ºå…œåº•ï¼‰----
            try:
                t0 = time.time()
                msg = self.llm.invoke(prompt)         # LangChain ChatOpenAI -> AIMessage
                gen_latency_ms = (time.time() - t0) * 1000.0

                answer_text = None
                # å¸¸è§„
                if hasattr(msg, "content"):
                    answer_text = msg.content
                # å…¼å®¹å…¶å®ƒå­—æ®µ
                if (not answer_text) and hasattr(msg, "message"):
                    answer_text = getattr(msg, "message")
                if (not answer_text) and hasattr(msg, "text"):
                    answer_text = getattr(msg, "text")
                # å…œåº•åˆ°å­—ç¬¦ä¸²
                if not answer_text:
                    answer_text = str(msg)

                # å¯èƒ½æ˜¯ list/å—ç»“æ„
                if isinstance(answer_text, (list, tuple)):
                    parts = []
                    for p in answer_text:
                        if isinstance(p, str):
                            parts.append(p)
                        elif isinstance(p, dict):
                            if "text" in p and isinstance(p["text"], str):
                                parts.append(p["text"])
                            elif "content" in p and isinstance(p["content"], str):
                                parts.append(p["content"])
                    answer_text = " ".join(parts).strip()

                answer_text = (answer_text or "").strip()
                if not answer_text:
                    answer_text = "[NO_ANSWER_GENERATED]"
            except Exception as e:
                gen_latency_ms = 0.0
                answer_text = "[NO_ANSWER_GENERATED]"
                if self.logger:
                    self.logger.add_reason(f"[gen.error] {type(e).__name__}: {e}")

            # ç”Ÿæˆæ—¥å¿—
            if self.logger:
                self.logger.add_generation(attempt=attempt + 1, prompt_id=prompt_id, answer=answer_text[:800])
                self.logger.add_eval(gen_latency_ms=round(gen_latency_ms, 2), attempt=attempt + 1)

            # ---- è¯„ä¼°ï¼ˆä¼  referenceï¼›æ‹¿åˆ†+statusï¼‰----
            try:
                eval_result = evaluation_agent.evaluate_generation(
                    user_query=question,
                    retrieved_docs=docs,
                    response=answer_text,
                    reference=ground_truth
                ) or {}
            except Exception as e:
                eval_result = {}
                if self.logger:
                    self.logger.add_reason(f"[eval.error] {type(e).__name__}: {e}")

            # åˆ†æ•°
            faithfulness_score = extract_scalar(eval_result.get("faithfulness", 0.0))
            relevancy_score_raw = eval_result.get("response_relevancy", None)
            if relevancy_score_raw is None:
                relevancy_score_raw = eval_result.get("answer_relevancy", 0.0)
            relevancy_score = extract_scalar(relevancy_score_raw)

            # statusï¼ˆé»˜è®¤missingï¼Œä¾¿äºåˆ¤æ–­ï¼‰
            faith_st = str(eval_result.get("faithfulness_status", "missing"))
            rel_st   = str(
                eval_result.get(
                    "response_relevancy_status",
                    eval_result.get("answer_relevancy_status", "missing")
                )
            )
            noise_st = "missing"

            # noise å…¼å®¹é”®å
            noise_sensitivity = None
            for k, v in eval_result.items():
                if "noise_sensitivity" in str(k):
                    noise_sensitivity = extract_scalar(v)
                    noise_st = str(
                        eval_result.get(f"{k}_status",
                        eval_result.get("noise_sensitivity_status", "ok"))
                    )
                    break
            if noise_sensitivity is None:
                noise_sensitivity, noise_st = 1.0, "missing"

            # ==== C. è¯­ä¹‰ F1ï¼ˆä¿®å¤ç‰ˆï¼‰ ====
            # å¯é€‰ï¼šè¯­ä¹‰ F1
            if self.semantic_f1_metric and ground_truth:
                try:
                    semantic_f1_score = self._safe_semantic_f1(str(ground_truth), str(answer_text))
                except Exception as e:
                    print(f"âš ï¸ Semantic F1 failed: {e}")
                    semantic_f1_score = 0.0
            else:
                semantic_f1_score = 0.0


            # è¯„ä¼°æ—¥å¿—ï¼ˆä¾¿äºå¤–éƒ¨æ’æŸ¥ï¼‰
            if self.logger:
                self.logger.add_eval(
                    faith=faithfulness_score, response_relevancy=relevancy_score,
                    noise_sensitivity=noise_sensitivity, semantic_f1=semantic_f1_score,
                    faith_status=faith_st, relevancy_status=rel_st, noise_status=noise_st,
                    attempt=attempt + 1
                )
                # è°ƒè¯•æ‰“å°ä¸€è¡Œï¼ˆå¯ç•™å¯å»ï¼‰
                print(f"ğŸ”§ eval_result -> faith={faithfulness_score:.4f}({faith_st}), "
                    f"rel={relevancy_score:.4f}({rel_st}), noise={noise_sensitivity:.4f}({noise_st}), "
                    f"ans[:60]={answer_text[:60]!r}")

            # ---- æ—©åœ or åˆ·æ–°æœ€ä½³ ----
            combined_score = self._compute_combined_score(
                faithfulness_score, relevancy_score, noise_sensitivity,
            )

            # æ£€ç´¢å·® â†’ é™æƒ
            if retr_prec < 0.2 or recall_like < 0.2:
                combined_score *= 0.6

            # â€œæœ‰ç”¨åˆ†æ•°â€ï¼šä»»ä¸€ status ä¸º ok å³è§†ä¸ºæœ‰æ•ˆè¯„æµ‹
            has_any_valid = (faith_st == "ok") or (rel_st == "ok") or (noise_st == "ok")

            # ==== B. æ—©åœï¼ˆæ£€ç´¢åˆæ ¼ + æ¨¡å‹åˆ†è¾¾æ ‡ï¼‰ ====
            early_stop = (
                has_any_valid and
                (retr_prec >= 0.5 and recall_like >= 0.5) and
                faithfulness_score >= 0.8 and
                relevancy_score   >= 0.6 and
                noise_sensitivity <= 0.4 and
                semantic_f1_score >= 0.7
            )
            if early_stop:
                if self.logger:
                    self.logger.set_final_answer(answer_text)
                return {
                    "answer": answer_text,
                    "faithfulness_score": faithfulness_score,
                    "response_relevancy": relevancy_score,
                    "answer_relevancy":  relevancy_score,   # é•œåƒ
                    "noise_sensitivity": noise_sensitivity,
                    "semantic_f1_score": semantic_f1_score,
                    "cached_eval_result": eval_result,
                    "eval_result": eval_result,
                    "faith": faithfulness_score,
                    "semantic_f1": semantic_f1_score,
                    "latency_ms": gen_latency_ms
                }

            # åˆ·æ–°æœ€ä½³ï¼šâ‘  æœ‰ä»»ä½•æœ‰æ•ˆåˆ†ä¸”æ›´ä¼˜ï¼›â‘¡ æˆ–â€œå°šæœªå†™å…¥è¿‡ best_answerâ€ï¼ˆé˜²ç©ºä¸²ï¼‰
            if (has_any_valid and combined_score > best_combined_score) or (not best_answer.strip()):
                if has_any_valid:
                    best_combined_score = combined_score
                best_answer = answer_text
                best_metrics = {
                    "faithfulness_score": faithfulness_score,
                    "response_relevancy": relevancy_score,
                    "answer_relevancy": relevancy_score,  # é•œåƒï¼Œæ–¹ä¾¿ä¸‹æ¸¸è¯»å–
                    "noise_sensitivity": noise_sensitivity,
                    "semantic_f1_score": semantic_f1_score
                }
                best_eval_result = eval_result
                best_latency_ms = gen_latency_ms

        # ---- è¾¾æœ€å¤§æ¬¡æ•°ï¼Œè¿”å›æœ€ä½³ ----
        if self.logger:
            self.logger.set_final_answer(best_answer)

        return {
            "answer": best_answer,
            "faithfulness_score": best_metrics.get("faithfulness_score", 0.0),
            "response_relevancy": best_metrics.get("response_relevancy", 0.0),
            "answer_relevancy":   best_metrics.get("answer_relevancy", best_metrics.get("response_relevancy", 0.0)),
            "noise_sensitivity": best_metrics.get("noise_sensitivity", 1.0),
            "semantic_f1_score": best_metrics.get("semantic_f1_score", 0.0),
            "cached_eval_result": best_eval_result,
            "eval_result": best_eval_result,
            "faith": best_metrics.get("faithfulness_score", 0.0),
            "semantic_f1": best_metrics.get("semantic_f1_score", 0.0),
            "latency_ms": best_latency_ms
        }

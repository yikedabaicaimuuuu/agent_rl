# agents/RLRouterAgent.py
# è¡Œä¸ºå…‹éš† (BC) è·¯ç”±å™¨ï¼šä» runs/trajectories/*.jsonl è¯»å–æ ·æœ¬è®­ç»ƒ MLPï¼Œ
# ä¸ langgraph_rag.py çš„ decision_state å­—æ®µä¸€ä¸€å¯¹åº”ã€‚

import os, json, glob
from typing import List, Dict, Any, Optional, Tuple

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler

# =========================
# è·¯å¾„ & å¸¸é‡
# =========================
TRAJ_DIR = os.getenv("TRAJ_DIR", os.path.join(os.path.dirname(__file__), "..", "runs", "trajectories"))
POLICY_SAVE_PATH = os.path.join(os.path.dirname(__file__), "router_policy.pt")

# ä¸ langgraph_rag.py çš„ decision_state é¡ºåºä¿æŒä¸€è‡´
FEATURE_KEYS = [
    "context_precision",     # ctxP
    "context_recall",        # ctxR
    "faithfulness_score",
    "response_relevancy",
    "noise_sensitivity",
    "semantic_f1_score",
]

ACTION2IDX = {"end": 0, "requery": 1, "regenerate": 2}
IDX2ACTION = {v: k for k, v in ACTION2IDX.items()}


# =========================
# å·¥å…·å‡½æ•°
# =========================
def _safe_float(x, default=0.0) -> float:
    try:
        if x is None:
            return default
        return float(x)
    except Exception:
        return default

def _coalesce_keys(rec: Dict[str, Any], keys: List[str], default=0.0) -> float:
    """ä»å¤šä¸ªå¯èƒ½é”®åé‡Œå–ä¸€ä¸ªå€¼ï¼ˆå…¼å®¹ä¸åŒå†™æ³•/å±‚çº§ï¼‰"""
    for k in keys:
        if k in rec:
            return _safe_float(rec[k], default)
    return default

def _extract_action(rec: Dict[str, Any]) -> Optional[int]:
    """ä»è½¨è¿¹è®°å½•ä¸­æŠ½å–åŠ¨ä½œï¼›å…¼å®¹ router_action / action_taken / action ç­‰å­—æ®µï¼Œå¹¶åšæ ‡å‡†åŒ–æ˜ å°„"""
    a = rec.get("router_action") or rec.get("action_taken") or rec.get("action")
    if not a:
        return None
    a = str(a).lower().strip()

    # ç»Ÿä¸€åˆ«å/ç»†ç²’åº¦åˆ° 3 ç±»
    if a in ("stop", "finish", "done", "end", "finalize"):
        a_std = "end"
    elif a in ("retry_retrieval", "requery", "retry", "retrieve_again"):
        a_std = "requery"
    elif a in ("regenerate", "regenerate_answer", "regenerate_answer_lower_temp", "regen", "rewrite_answer"):
        a_std = "regenerate"
    else:
        # æœªçŸ¥åŠ¨ä½œå…¨éƒ¨å½’å¹¶ä¸º end
        a_std = "end"

    return ACTION2IDX.get(a_std, ACTION2IDX["end"])


# >>> CHANGE: å¢åŠ æ•™å¸ˆè§„åˆ™ï¼ˆç”¨äºè‡ªåŠ¨æ‰“æ ‡ç­¾ & æ¨ç†å›é€€ï¼‰
def _teacher_rule_action(metrics_like: Dict[str, float]) -> str:
    """
    è¾“å…¥å¯ä»¥æ˜¯ rec æˆ– rec['eval'] æˆ–ä½ ç»„è£…çš„ metricsã€‚
    åªä¾èµ– 6 ä¸ªç‰¹å¾ï¼šctxP, ctxR, faith, rel, noise, semf1
    """
    def f(*names, default=0.0):
        for n in names:
            if n in metrics_like:
                return _safe_float(metrics_like[n], default)
        return default

    ctxR  = f("context_recall", "ctxR", default=0.0)
    rel   = f("response_relevancy", "answer_relevancy", default=0.0)
    faith = f("faithfulness_score", "faith", "faithfulness", default=0.0)
    noise = f("noise_sensitivity", "noise", default=1.0)
    semf1 = f("semantic_f1_score", "semantic_f1", default=0.0)

    # è§„åˆ™ä¸æˆ‘ä»¬å‰é¢è®¨è®ºä¿æŒä¸€è‡´ï¼ˆä½ å¯ä»¥æŒ‰éœ€å¾®è°ƒï¼‰ï¼š
    if ctxR <= 1e-6:
        return "requery"
    if rel < 0.40:
        return "requery"            # ä½ çš„ä¸‰åŠ¨ä½œé›†åˆé‡Œæ²¡æœ‰â€œæ”¹å†™åé‡è¯•â€ï¼Œè¿™é‡Œç»Ÿä¸€å½’åˆ° requery
    if (faith < 0.55) or (noise > 0.60):
        return "regenerate"
    if semf1 >= 0.70:
        return "end"
    return "regenerate"

def _minmax_norm(x: torch.Tensor, fmin: torch.Tensor, fmax: torch.Tensor) -> torch.Tensor:
    denom = torch.clamp(fmax - fmin, min=1e-6)
    return (x - fmin) / denom

def _clamp01(x: torch.Tensor) -> torch.Tensor:
    return torch.clamp(x, 0.0, 1.0)


# =========================
# æ•°æ®é›†ï¼šä» JSONL è½¨è¿¹æŠ½æ ·
# =========================
class RouterTrajectoryDataset(Dataset):
    """
    ä» runs/trajectories/*.jsonl é‡Œé€æ¡è¯»å–è½¨è¿¹ï¼›
    æ¯æ¡æ ·æœ¬ä½¿ç”¨â€œæœ€æ–°è¯„ä¼°æŒ‡æ ‡ + æœ€ç»ˆè·¯ç”±åŠ¨ä½œâ€ä½œä¸ºè®­ç»ƒå¯¹ã€‚
    """
    def __init__(self, traj_dir: str = TRAJ_DIR):
        self.samples: List[Tuple[List[float], int]] = []
        self._load(traj_dir)

    def _load(self, traj_dir: str):
        if not os.path.isdir(traj_dir):
            raise FileNotFoundError(f"Trajectory directory not found: {traj_dir}")

        paths = sorted(glob.glob(os.path.join(traj_dir, "*.jsonl")))
        if not paths:
            raise FileNotFoundError(f"No JSONL files under: {traj_dir}")

        for p in paths:
            with open(p, "r", encoding="utf-8") as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    try:
                        rec = json.loads(line)
                    except Exception:
                        continue

                    container = rec.get("eval", rec)

                    # æŒ‡æ ‡æŠ½å–ï¼ˆå…¨éƒ¨å°½é‡è½åœ¨ 0..1ï¼Œç¼ºçœç»™ 0/1 åˆç†å€¼ï¼‰
                    ctxP  = _coalesce_keys(container, ["ctxP", "context_precision"], default=0.0)
                    ctxR  = _coalesce_keys(container, ["ctxR", "context_recall"],    default=0.0)
                    faith = _coalesce_keys(container, ["faith", "faithfulness", "faithfulness_score"], default=0.0)
                    rel   = _coalesce_keys(container, ["response_relevancy", "answer_relevancy"], default=0.0)
                    noise = _coalesce_keys(container, ["noise_sensitivity", "noise"], default=1.0)
                    semf1 = _coalesce_keys(container, ["semantic_f1", "semantic_f1_score"], default=0.0)

                    feats = [ctxP, ctxR, faith, rel, noise, semf1]

                    # 1) è½¨è¿¹é‡Œå·²æœ‰åŠ¨ä½œ â†’ ç”¨å®ƒ
                    action_idx = _extract_action(rec)
                    # >>> CHANGE: 2) å¦åˆ™è‡ªåŠ¨ç”¨æ•™å¸ˆè§„åˆ™æ‰“æ ‡ç­¾ï¼ˆä¸å†ä¸¢æ ·æœ¬ï¼‰
                    if action_idx is None:
                        a = _teacher_rule_action(container)
                        action_idx = ACTION2IDX.get(a, ACTION2IDX["end"])

                    self.samples.append((feats, action_idx))

        if not self.samples:
            raise RuntimeError("No valid samples found; run the pipeline to generate trajectories first.")

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        feats, action_idx = self.samples[idx]
        return torch.tensor(feats, dtype=torch.float32), torch.tensor(action_idx, dtype=torch.long)


# =========================
# MLP ç­–ç•¥ç½‘ç»œ
# =========================
class RouterPolicyNet(nn.Module):
    def __init__(self, input_dim: int = len(FEATURE_KEYS), hidden_dim: int = 32, num_actions: int = 3):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, num_actions)
        )

    def forward(self, x):
        return self.net(x)


# =========================
# è®­ç»ƒï¼ˆè¡Œä¸ºå…‹éš†ï¼‰
# =========================
def train_router(
    traj_dir: str = TRAJ_DIR,
    epochs: int = 20,
    batch_size: int = 16,
    lr: float = 1e-3,
    class_balance: bool = True,          # ç±»åˆ«ä¸å¹³è¡¡æ—¶è‡ªåŠ¨ reweight
    save_path: str = POLICY_SAVE_PATH,
    device: str = "cpu"
):
    dataset = RouterTrajectoryDataset(traj_dir)

    # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒï¼ˆç”¨äºæ—¥å¿—ï¼‰
    labels_all = torch.tensor([dataset[i][1] for i in range(len(dataset))], dtype=torch.long)
    class_counts = torch.bincount(labels_all, minlength=len(ACTION2IDX)).clamp(min=1)
    print(f"[train] class counts: " + ", ".join(f"{IDX2ACTION[i]}={int(c)}" for i, c in enumerate(class_counts)))

    # è®¡ç®—ç‰¹å¾ min/maxï¼ˆç”¨äºä¿å­˜åˆ° ckptï¼Œæ¨ç†æ—¶å¤ç”¨ï¼‰
    all_feats = torch.stack([dataset[i][0] for i in range(len(dataset))], dim=0)  # [N, D]
    feat_min = all_feats.min(dim=0, keepdim=True).values
    feat_max = all_feats.max(dim=0, keepdim=True).values
    norm_feats = _minmax_norm(all_feats, feat_min, feat_max)

    class _NormedDS(Dataset):
        def __init__(self, feats: torch.Tensor, labels: torch.Tensor):
            self._feats = feats
            self._labels = labels
        def __len__(self): return self._feats.size(0)
        def __getitem__(self, idx): return self._feats[idx], self._labels[idx]

    ds = _NormedDS(norm_feats, labels_all)

    # ç±»åˆ«ä¸å¹³è¡¡é‡‡æ ·
    if class_balance:
        class_weights = 1.0 / class_counts.float()
        sample_weights = class_weights[labels_all]
        sampler = WeightedRandomSampler(sample_weights, num_samples=len(ds), replacement=True)
        loader = DataLoader(ds, batch_size=batch_size, sampler=sampler)
    else:
        loader = DataLoader(ds, batch_size=batch_size, shuffle=True)

    model = RouterPolicyNet().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=lr)

    model.train()
    for epoch in range(epochs):
        total_loss = 0.0
        total_correct = 0
        total_seen = 0

        for feats, actions in loader:
            feats, actions = feats.to(device), actions.to(device)
            logits = model(feats)
            loss = criterion(logits, actions)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            total_loss += loss.item() * feats.size(0)
            pred = torch.argmax(logits, dim=-1)
            total_correct += (pred == actions).sum().item()
            total_seen += actions.numel()

        avg_loss = total_loss / max(1, total_seen)
        acc = total_correct / max(1, total_seen)
        print(f"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Acc: {acc:.4f}")

    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    torch.save(
        {
            "state_dict": model.state_dict(),
            "feat_min": feat_min.cpu(),
            "feat_max": feat_max.cpu(),
            "feature_keys": FEATURE_KEYS,
            "action2idx": ACTION2IDX,
        },
        save_path
    )
    print(f"âœ… Trained policy saved to {save_path}")


# =========================
# æ¨ç†å°è£…
# =========================
class RLRouterAgent:
    def __init__(self, policy_path: Optional[str] = None, device: str = "cpu", logger=None):
        self.device = device
        self.logger = logger
        self.policy = RouterPolicyNet().to(self.device)

        self.feat_min: Optional[torch.Tensor] = None  # [1, D]
        self.feat_max: Optional[torch.Tensor] = None  # [1, D]
        self.policy_path: Optional[str] = policy_path
        self._has_policy: bool = False                 # âœ… æ–°å¢ï¼šè®°å½•æ˜¯å¦å·²æˆåŠŸåŠ è½½

        # ==== æ–°å¢ï¼šç”¨ç¯å¢ƒå˜é‡æ§åˆ¶æ¨¡å¼ ====
        # ROUTER_MODE å¯é€‰ï¼š "off" / "teacher" / "bc"
        self.mode = os.getenv("ROUTER_MODE", "bc").lower()
        if self.mode not in ("off", "teacher", "bc"):
            self.mode = "bc"

        # ---- off æ¨¡å¼ï¼šrouter å­˜åœ¨ï¼Œä½†æ°¸è¿œç›´æ¥ end ----
        if self.mode == "off":
            print("ğŸ§  RLRouterAgent in OFF mode (ROUTER_MODE=off): always choose 'end'.")
            self.policy.eval()
            return

        # ---- teacher æ¨¡å¼ï¼šå®Œå…¨ä¸ç”¨ BC policyï¼Œåªèµ°æ•™å¸ˆè§„åˆ™ ----
        if self.mode == "teacher":
            print("ğŸ§  RLRouterAgent in TEACHER-RULE mode (ROUTER_MODE=teacher); ignore policy file.")
            self.policy.eval()
            return

        # ---- bc æ¨¡å¼ï¼šæ­£å¸¸åŠ è½½ policy, å¤±è´¥åˆ™å›é€€ teacher rule----
        if policy_path and os.path.exists(policy_path):
            ckpt = torch.load(policy_path, map_location=self.device)
            state_dict = ckpt.get("state_dict", ckpt)
            self.policy.load_state_dict(state_dict, strict=False)

            if "feat_min" in ckpt and "feat_max" in ckpt:
                self.feat_min = ckpt["feat_min"].to(self.device).float()
                self.feat_max = ckpt["feat_max"].to(self.device).float()
            else:
                print("âš ï¸ No feat_min/feat_max in checkpoint; will fall back to clamp(0..1).")

            print(f"Loaded router policy from {policy_path}")
            self._has_policy = True                     # âœ… æˆåŠŸåŠ è½½
        else:
            # ä¸å­˜åœ¨æˆ–æœªæä¾› policy_path
            if policy_path:
                print(f"âš ï¸ Router policy not found at: {policy_path}; will fallback to teacher rules.")
            else:
                print("âš ï¸ No trained router policy found; will fallback to teacher rules.")
        self.policy.eval()

    def _featurize(self, state: Dict[str, float]) -> torch.Tensor:
        """æŒ‰ FEATURE_KEYS é¡ºåºå–ç‰¹å¾ï¼Œå¹¶åšä¸è®­ç»ƒä¸€è‡´çš„å½’ä¸€åŒ–ã€‚"""
        vals: List[float] = []
        # å…¼å®¹ ctxP/ctxR å‘½å
        cp = state.get("context_precision", state.get("ctxP", 0.0))
        cr = state.get("context_recall", state.get("ctxR", 0.0))
        vals.append(_safe_float(cp))
        vals.append(_safe_float(cr))
        vals.append(_safe_float(state.get("faithfulness_score", state.get("faith", 0.0))))
        vals.append(_safe_float(state.get("response_relevancy", state.get("answer_relevancy", 0.0))))
        vals.append(_safe_float(state.get("noise_sensitivity", state.get("noise", 1.0))))
        vals.append(_safe_float(state.get("semantic_f1_score", state.get("semantic_f1", 0.0))))

        x = torch.tensor(vals, dtype=torch.float32, device=self.device).unsqueeze(0)  # [1, D]
        if self.feat_min is not None and self.feat_max is not None:
            x = _minmax_norm(x, self.feat_min, self.feat_max)
        else:
            x = _clamp01(x)
        return x

    def decide(self, state: Dict[str, float], greedy: bool = True, temperature: float = 1.0) -> str:
        # å¦‚æœæ²¡æ¨¡å‹ï¼Œç›´æ¥æ•™å¸ˆè§„åˆ™ï¼›è¿™æ · 3 ä¸ª case ä¹Ÿèƒ½å®Œæ•´èµ°é€š
        # âœ… ç”¨åŠ è½½ç»“æœåˆ¤æ–­ï¼Œè€Œéç¡¬è¯»å…¨å±€å¸¸é‡è·¯å¾„
        # 1) off æ¨¡å¼ï¼šæ°¸è¿œç›´æ¥ endï¼Œç›¸å½“äºâ€œæœ‰ graph ä½† router ä¸å¹²é¢„â€
        if getattr(self, "mode", "bc") == "off":
            action = "end"
            print("[router.decide] ROUTER_MODE=off â†’ always 'end'")
            if self.logger:
                self.logger.set_router_action(action)
            return action

        # 2) teacher æ¨¡å¼ æˆ– æ²¡æœ‰æˆåŠŸåŠ è½½ policyï¼šç”¨æ•™å¸ˆè§„åˆ™
        if self.mode == "teacher" or not getattr(self, "_has_policy", False):
            if self.mode == "teacher":
                print("[router.decide] ROUTER_MODE=teacher â†’ teacher_rule")
            else:
                print("[router.decide] no loaded policy â†’ fallback teacher_rule")
            a = _teacher_rule_action(state)
            if self.logger:
                self.logger.set_router_action(a)
            return a

        # 3) bc æ¨¡å¼ + å·²æœ‰ policyï¼šç”¨ MLP policy
        with torch.no_grad():
            x = self._featurize(state)
            logits = self.policy(x)

            if greedy or temperature <= 0:
                action_idx = torch.argmax(logits, dim=-1).item()
            else:
                probs = torch.softmax(logits / max(1e-6, float(temperature)), dim=-1).squeeze(0).cpu().numpy()
                action_idx = int(probs.argmax())

        a = IDX2ACTION.get(action_idx, "end")
        print(f"[router.decide] ROUTER_MODE=bc + BC POLICY action={a} (idx={action_idx})")
        if self.logger:
            self.logger.set_router_action(a)
        return a


# =========================
# å‘½ä»¤è¡Œè®­ç»ƒå…¥å£ï¼ˆå¯é€‰ï¼‰
# =========================
if __name__ == "__main__":
    # ç›´æ¥ï¼špython agents/RLRouterAgent.py
    train_router()
